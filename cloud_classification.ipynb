{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owjwav1EyO1R",
        "colab_type": "code",
        "outputId": "8421ffb1-33c4-4063-c910-6ff76e0ee1b8",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@title { form-width: \"5%\" }\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Concatenate, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "from random import randrange\n",
        "import pandas as pd\n",
        " \n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/Project ML\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Project ML\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-pAJd18t3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Load Training Data with Returning Features and Labels\"\"\"\n",
        "\n",
        "def load_data_dummy(size):\n",
        "  #load data\n",
        "\n",
        "  images_path = os.listdir('progress 1/dummy_data/')\n",
        "  \n",
        "  label_map = {'free':[1,0,0],'thin':[0,1,0],'thick': [0,0,1]}\n",
        "  feature = []\n",
        "  label = []\n",
        "\n",
        "  for file_name in images_path:\n",
        "    \n",
        "    # Read image file\n",
        "    temp = cv2.imread('progress 1/dummy_data/' + file_name)\n",
        "    \n",
        "    # Resize image\n",
        "    im_resize = cv2.resize(temp,(size,size))\n",
        "    feature.append(im_resize)\n",
        "    \n",
        "    # Append labels.\n",
        "    label.append(label_map[file_name.split('_')[0]])\n",
        "   \n",
        "  label = np.array(label).astype(float)\n",
        "  return np.array(feature) / 255.0, label\n",
        "\n",
        "def load_data(size, test_pct = 0.2):\n",
        "  #load data\n",
        "  \n",
        "  prefix_dir = 'data/size_500/'\n",
        "\n",
        "  target_path = os.listdir(prefix_dir)\n",
        "  \n",
        "  label_map = {'free':[1,0,0],'thin':[0,1,0],'thick': [0,0,1]}\n",
        "  feature = []\n",
        "  label = []\n",
        "  for target in target_path:\n",
        "    # List images\n",
        "    images_path = os.listdir(prefix_dir + target)\n",
        "    \n",
        "    for file_name in tqdm(images_path):\n",
        "      \n",
        "      # Read image file\n",
        "      temp = cv2.imread(prefix_dir + target + '/' + file_name)\n",
        "      \n",
        "      # Resize image\n",
        "      im_resize = cv2.resize(temp,(size,size))\n",
        "      feature.append(im_resize)\n",
        "\n",
        "      # Append labels.\n",
        "      label.append(label_map[target])\n",
        "  \n",
        "  # Convert to numpy array\n",
        "  train_feature = np.array(feature) / 255.0\n",
        "  train_label = np.array(label).astype(float)\n",
        "  \n",
        "  # Get test data\n",
        "  test_index = random.sample(range(len(label)), int(len(label) * test_pct))\n",
        "  test_feature = train_feature[test_index]\n",
        "  test_label = train_label[test_index]\n",
        "  \n",
        "  # Delete test data from train data\n",
        "  train_feature = np.delete(train_feature, test_index, 0) \n",
        "  train_label = np.delete(train_label, test_index, 0)\n",
        "  \n",
        "  return train_feature, train_label, test_feature, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tioUAfVnZkuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, folds=3):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / folds)\n",
        "\tfor i in range(folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heuOM4SL-F_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_model_fn(num_filter=[6,16], size_kernels = [5,5]):\n",
        "  model = Sequential()\n",
        "  #Layer 1\n",
        "  #Conv Layer 1\n",
        "  model.add(Conv2D(filters = num_filter[0], \n",
        "                   kernel_size = size_kernels[0], \n",
        "                   strides = 1, \n",
        "                   activation = 'relu', \n",
        "                   input_shape = (128,128,3)))\n",
        "  \n",
        "\n",
        "  #Pooling layer 1\n",
        "  model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "  #Layer 2\n",
        "  #Conv Layer 2\n",
        "  model.add(Conv2D(filters = num_filter[1], \n",
        "                   kernel_size = size_kernels[0],\n",
        "                   strides = 1,\n",
        "                   activation = 'relu',))\n",
        "                   #input_shape = (64,64,6)))\n",
        "  \n",
        "  #Pooling Layer 2\n",
        "  model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "  #Flatten\n",
        "  model.add(Flatten())\n",
        "  #Layer 3\n",
        "  #Fully connected layer 1\n",
        "  model.add(Dense(units = 120, activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.2))\n",
        "  #Layer 4\n",
        "  #Fully connected layer 2\n",
        "  model.add(Dense(units = 84, activation = 'relu'))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.2))\n",
        "  #Layer 5\n",
        "  #Output Layer\n",
        "  model.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc50srBoCi9D",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title { form-width: \"5%\" }\n",
        "\n",
        "#plot loss and acc\n",
        "def plotLoss(history):\n",
        "\n",
        "    #figname = \"b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str( foldind) + \"_\" + str(time.time()) \n",
        "    figname = 'result'\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\t  #plt.legend(['trainloss', 'valloss', 'cindex', 'valcindex'], loc='upper left')\n",
        "    plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
        "    plt.savefig(figname +\".png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait', \n",
        "                    papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    ## PLOT CINDEX\n",
        "    plt.figure()\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('acc')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
        "    plt.savefig(figname + \"_acc.png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait', \n",
        "                            papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7tACoGEcnT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title { form-width: \"5%\" }\n",
        "\n",
        "# Create a TensorBoard instance with the path to the logs directory\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
        "\n",
        "# define the checkpoint\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath = \"checkpoint/model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0MPzjD3EjeR",
        "colab_type": "code",
        "outputId": "8e23481e-55cb-406d-a6c8-2406adbeb6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 31193
        }
      },
      "source": [
        "# load all data\n",
        "data, label, _ , _ = load_data(128, 0)\n",
        "\n",
        "# Cross validation, generate index for each fold\n",
        "folds = cross_validation_split(range(len(data)),3)\n",
        "\n",
        "# Grid Search for number of filters and kernel size\n",
        "num_filter_list = [[6,16]]\n",
        "kernel_size_list = [[5,5]]\n",
        "\n",
        "result = []\n",
        "for num_filter in num_filter_list:\n",
        "  for kernel_size in kernel_size_list:\n",
        "    \n",
        "    # Train for each fold\n",
        "    for fold_id in range(len(folds)):\n",
        "      \n",
        "      # Get test data for current fold\n",
        "      test_id = folds[fold_id]\n",
        "      test_data = data[test_id]\n",
        "      test_label = label[test_id]\n",
        "      \n",
        "      \n",
        "      # Get train data\n",
        "      train_id = []\n",
        "      for i in range(len(folds)):\n",
        "        if i != test_id:\n",
        "          train_id += folds[i]\n",
        "      print(train_id)\n",
        "      \n",
        "      train_data = data[train_id]\n",
        "      train_label = label[train_id]\n",
        "      \n",
        "      \n",
        "      # Build model\n",
        "      model = cnn_model_fn(num_filter, kernel_size) \n",
        "      model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "      # Training\n",
        "      model.fit(train_data ,train_label, batch_size = 64, epochs = 500, validation_data=(test_data,test_label))#, callbacks=[tensorboard, checkpoint])\n",
        "      \n",
        "      # Plot loss and accuracy graph\n",
        "      plotLoss(model.history)\n",
        "\n",
        "      # Testing\n",
        "      scores = model.evaluate(test_data, test_label, batch_size=1, verbose=1)\n",
        "      print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n",
        "      print(scores)\n",
        "      result.append([num_filter, kernel_size, test_id, scores[1]*100])\n",
        "\n",
        "\n",
        "result = pd.DataFrame(result, columns = ['num_filter', 'kernel_size','fold', 'acc'])\n",
        "result.to_csv('result.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:15<00:00,  1.42it/s]\n",
            "100%|██████████| 500/500 [06:38<00:00,  1.06it/s]\n",
            "100%|██████████| 150/150 [01:51<00:00,  1.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[242, 96, 315, 115, 260, 556, 284, 652, 363, 814, 118, 524, 433, 339, 434, 272, 28, 445, 698, 1054, 633, 772, 580, 417, 231, 79, 235, 169, 318, 7, 332, 644, 355, 86, 99, 824, 709, 727, 391, 908, 703, 43, 658, 680, 215, 1033, 638, 1079, 269, 63, 740, 258, 457, 1036, 176, 171, 730, 1044, 960, 91, 1014, 549, 309, 420, 1116, 308, 82, 428, 77, 546, 468, 883, 783, 968, 422, 155, 682, 606, 40, 425, 543, 808, 334, 504, 536, 802, 18, 253, 752, 498, 929, 283, 376, 328, 412, 780, 27, 201, 1037, 962, 305, 285, 859, 641, 884, 1100, 892, 932, 237, 399, 233, 767, 175, 10, 1129, 500, 1025, 1107, 1114, 818, 1072, 766, 162, 687, 639, 451, 1134, 961, 891, 834, 851, 436, 689, 805, 866, 1005, 94, 852, 393, 67, 371, 710, 1142, 1083, 938, 729, 73, 656, 950, 815, 54, 470, 980, 1135, 953, 1098, 967, 218, 538, 595, 888, 496, 1080, 914, 898, 1020, 845, 762, 191, 982, 47, 1126, 986, 746, 530, 35, 857, 711, 820, 676, 781, 981, 831, 107, 145, 754, 427, 572, 700, 1009, 261, 792, 165, 435, 1048, 45, 78, 511, 1028, 121, 513, 405, 1024, 901, 939, 696, 312, 790, 577, 653, 1102, 12, 98, 521, 1104, 313, 552, 172, 185, 486, 60, 509, 303, 586, 15, 736, 72, 750, 22, 390, 192, 379, 429, 144, 1143, 462, 678, 1002, 317, 630, 326, 817, 679, 8, 46, 525, 227, 881, 856, 59, 623, 742, 402, 1140, 401, 471, 765, 970, 941, 758, 1032, 252, 182, 635, 775, 579, 216, 830, 640, 523, 646, 1064, 949, 302, 167, 1039, 89, 226, 671, 163, 728, 514, 1108, 1112, 683, 208, 1057, 522, 323, 844, 301, 338, 673, 426, 14, 126, 136, 1022, 1074, 494, 578, 739, 129, 564, 454, 560, 789, 608, 1138, 836, 706, 557, 990, 988, 195, 1053, 675, 288, 199, 5, 721, 734, 839, 667, 29, 672, 900, 649, 225, 975, 664, 212, 450, 600, 915, 763, 44, 197, 1063, 36, 80, 878, 306, 352, 800, 141, 277, 499, 576, 947, 1011, 465, 642, 1096, 544, 24, 456, 518, 259, 478, 614, 441, 571, 349, 385, 184, 904, 276, 945, 58, 1088, 1127, 1000, 519, 411, 1034, 542, 295, 1110, 533, 1021, 17, 745, 1092, 813, 13, 321, 491, 1068, 551, 460, 251, 1006, 713, 1149, 751, 1123, 69, 965, 942, 704, 173, 694, 105, 370, 607, 293, 492, 418, 374, 992, 690, 702, 482, 931, 254, 1038, 773, 469, 206, 849, 554, 733, 416, 66, 624, 166, 985, 613, 1111, 620, 324, 1090, 663, 930, 605, 872, 1119, 677, 684, 593, 539, 241, 347, 369, 344, 959, 629, 298, 726, 599, 316, 64, 693, 660, 691, 760, 612, 84, 240, 95, 357, 360, 840, 489, 951, 250, 159, 737, 1062, 160, 843, 784, 1085, 974, 944, 893, 25, 545, 846, 590, 862, 812, 362, 906, 987, 52, 976, 279, 1139, 310, 238, 220, 1109, 1141, 9, 214, 566, 351, 928, 410, 1124, 407, 1076, 190, 1084, 822, 618, 731, 761, 943, 404, 1045, 759, 913, 865, 1023, 716, 101, 377, 437, 93, 670, 969, 256, 459, 148, 1121, 120, 70, 281, 826, 267, 627, 526, 146, 777, 979, 380, 1148, 83, 507, 273, 529, 373, 149, 239, 534, 924, 1106, 353, 100, 1007, 911, 188, 810, 200, 648, 383, 674, 714, 178, 559, 473, 56, 875, 948, 221, 274, 1137, 234, 193, 452, 885, 495, 978, 535, 325, 1056, 453, 408, 591, 963, 661, 257, 705, 396, 204, 346, 1094, 3, 594, 643, 113, 1086, 278, 909, 637, 31, 1041, 569, 388, 194, 1073, 1043, 589, 550, 458, 776, 1016, 291, 565, 71, 547, 1029, 515, 659, 853, 512, 398, 744, 528, 827, 207, 23, 619, 87, 785, 488, 615, 804, 686, 85, 158, 282, 747, 484, 1047, 400, 104, 92, 997, 345, 1105, 842, 481, 392, 854, 863, 516, 122, 187, 50, 610, 290, 419, 359, 994, 1018, 855, 53, 1042, 42, 860, 636, 1118, 907, 111, 841, 55, 749, 867, 647, 541, 112, 109, 442, 6, 570, 387, 382, 879, 510, 1070, 222, 1058, 584, 130, 874, 695, 75, 1115, 156, 1120, 320, 935, 771, 574, 779, 583, 952, 791, 403, 341, 266, 139, 617, 520, 896, 271, 732, 406, 558, 461, 717, 1132, 738, 1065, 770, 1089, 114, 337, 1059, 625, 322, 555, 333, 413, 133, 764, 1026, 41, 864, 384, 343, 1031, 74, 701, 174, 966, 223, 787, 476, 654, 902, 247, 668, 837, 181, 21, 793, 137, 933, 76, 119, 30, 838, 786, 116, 655, 164, 925, 38, 485, 621, 517, 681, 768, 125, 472, 921, 1078, 372, 300, 423, 972, 723, 897, 850, 871, 1125, 168, 151, 795, 236, 801, 748, 1103, 327, 1136, 350, 912, 934, 889, 877, 0, 958, 886, 993, 1069, 255, 103, 1061, 299, 708, 463, 597, 20, 249, 622, 479, 330, 62, 102, 955, 567, 1144, 152, 1113, 984, 131, 585, 33, 870, 132, 973, 1131, 719, 910, 645, 1010, 741, 211, 110, 697, 563, 936, 348, 368, 490, 828, 905, 232, 946, 531, 311, 601, 127, 720, 202, 847, 725, 210, 124, 1012, 1082, 475, 2, 631, 474, 356, 503, 354, 394, 440, 39, 180, 587, 1117, 32, 1001, 157, 268, 832, 263, 106, 395, 650, 209, 297, 444, 999, 537, 424, 548, 688, 161, 890, 823, 19, 361, 331, 464, 1093, 882, 432, 797, 198, 753, 665, 366, 956, 923, 378, 1060, 262, 575, 991, 448, 189, 477, 340, 540, 611, 147, 1122, 1, 604, 1027, 873, 669, 88, 142, 920, 774, 430, 1019, 493, 940, 1071, 917, 397, 264, 1051, 582, 1013, 743, 205, 1055, 1066, 811, 443, 287, 286, 505, 81, 1052, 480, 265, 735, 1095, 16, 117, 1099, 1091, 918, 245, 829, 788, 937, 364, 1049, 389, 438, 803, 68, 926, 186, 170, 957, 876, 998, 778, 699, 280, 592, 634, 927, 506, 598, 903, 616, 228, 562, 466, 319, 138, 1017, 123, 467, 217, 809, 983, 869, 1130, 37, 48, 755, 977, 365, 848, 487, 203, 835, 229, 1146, 179, 381, 568, 561, 899, 183, 196, 367, 244, 219, 51, 342, 919, 246, 57, 527, 989, 386, 154, 782, 718, 651, 335, 177, 588, 858, 581, 825, 1097, 553, 446, 657, 996, 916, 1050, 483, 294, 833, 1046, 134, 819, 150, 90, 662, 806, 414, 887, 447, 821, 296, 685, 336, 415, 11, 135, 224, 1067, 1133, 153, 1145, 1008, 230, 1128, 798, 1087, 724, 1081, 502, 314, 894, 757, 49, 816, 304, 573, 275, 143, 1040, 34, 421, 715, 1077, 140, 602, 794, 243, 1035, 431, 964, 1004, 603, 292, 1075, 895, 97, 666, 609, 722, 807, 799, 289, 497, 692, 61, 1015, 880, 108, 270, 248, 128, 861, 626, 439, 329, 922, 4, 1147, 712, 796, 707, 307, 532, 628, 358, 971, 632, 375, 1003, 756, 501, 868, 1101, 954, 1030, 995, 769, 409, 213, 596, 449, 65, 455, 26]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 124, 124, 6)       456       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 62, 62, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 58, 58, 16)        2416      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 13456)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               1614840   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 84)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 255       \n",
            "=================================================================\n",
            "Total params: 1,628,131\n",
            "Trainable params: 1,628,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1149 samples, validate on 383 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/500\n",
            "1149/1149 [==============================] - 4s 4ms/sample - loss: 0.9341 - acc: 0.4543 - val_loss: 0.8522 - val_acc: 0.4308\n",
            "Epoch 2/500\n",
            "1149/1149 [==============================] - 1s 756us/sample - loss: 0.7797 - acc: 0.5692 - val_loss: 0.6901 - val_acc: 0.4726\n",
            "Epoch 3/500\n",
            "1149/1149 [==============================] - 1s 761us/sample - loss: 0.5370 - acc: 0.8024 - val_loss: 0.3664 - val_acc: 0.8538\n",
            "Epoch 4/500\n",
            "1149/1149 [==============================] - 1s 766us/sample - loss: 0.3715 - acc: 0.8451 - val_loss: 0.3226 - val_acc: 0.8433\n",
            "Epoch 5/500\n",
            "1149/1149 [==============================] - 1s 761us/sample - loss: 0.2819 - acc: 0.8738 - val_loss: 0.1971 - val_acc: 0.9191\n",
            "Epoch 6/500\n",
            "1149/1149 [==============================] - 1s 788us/sample - loss: 0.2609 - acc: 0.8930 - val_loss: 0.2328 - val_acc: 0.9008\n",
            "Epoch 7/500\n",
            "1149/1149 [==============================] - 1s 807us/sample - loss: 0.2001 - acc: 0.9130 - val_loss: 0.1409 - val_acc: 0.9504\n",
            "Epoch 8/500\n",
            "1149/1149 [==============================] - 1s 784us/sample - loss: 0.2124 - acc: 0.9121 - val_loss: 0.1442 - val_acc: 0.9426\n",
            "Epoch 9/500\n",
            "1149/1149 [==============================] - 1s 790us/sample - loss: 0.1924 - acc: 0.9173 - val_loss: 0.2396 - val_acc: 0.8851\n",
            "Epoch 10/500\n",
            "1149/1149 [==============================] - 1s 776us/sample - loss: 0.1981 - acc: 0.9182 - val_loss: 0.1394 - val_acc: 0.9478\n",
            "Epoch 11/500\n",
            "1149/1149 [==============================] - 1s 789us/sample - loss: 0.1855 - acc: 0.9138 - val_loss: 0.2218 - val_acc: 0.8903\n",
            "Epoch 12/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 0.1805 - acc: 0.9225 - val_loss: 0.1115 - val_acc: 0.9452\n",
            "Epoch 13/500\n",
            "1149/1149 [==============================] - 1s 798us/sample - loss: 0.1217 - acc: 0.9591 - val_loss: 0.0859 - val_acc: 0.9661\n",
            "Epoch 14/500\n",
            "1149/1149 [==============================] - 1s 781us/sample - loss: 0.1049 - acc: 0.9574 - val_loss: 0.0729 - val_acc: 0.9817\n",
            "Epoch 15/500\n",
            "1149/1149 [==============================] - 1s 790us/sample - loss: 0.1825 - acc: 0.9347 - val_loss: 0.2191 - val_acc: 0.9164\n",
            "Epoch 16/500\n",
            "1149/1149 [==============================] - 1s 799us/sample - loss: 0.1204 - acc: 0.9547 - val_loss: 0.1111 - val_acc: 0.9399\n",
            "Epoch 17/500\n",
            "1149/1149 [==============================] - 1s 839us/sample - loss: 0.1900 - acc: 0.9312 - val_loss: 0.2186 - val_acc: 0.9138\n",
            "Epoch 18/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 0.2828 - acc: 0.8869 - val_loss: 0.1838 - val_acc: 0.9426\n",
            "Epoch 19/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 0.1832 - acc: 0.9208 - val_loss: 0.2511 - val_acc: 0.8564\n",
            "Epoch 20/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 0.2112 - acc: 0.9077 - val_loss: 0.1604 - val_acc: 0.9164\n",
            "Epoch 21/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 0.1399 - acc: 0.9469 - val_loss: 0.0834 - val_acc: 0.9791\n",
            "Epoch 22/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 0.1404 - acc: 0.9460 - val_loss: 0.0851 - val_acc: 0.9843\n",
            "Epoch 23/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 0.1056 - acc: 0.9591 - val_loss: 0.0674 - val_acc: 0.9765\n",
            "Epoch 24/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 0.0937 - acc: 0.9591 - val_loss: 0.2210 - val_acc: 0.8877\n",
            "Epoch 25/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 0.1256 - acc: 0.9504 - val_loss: 0.1796 - val_acc: 0.9112\n",
            "Epoch 26/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 6.3175 - acc: 0.5770 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 27/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 28/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 29/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 30/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 31/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 32/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 33/500\n",
            "1149/1149 [==============================] - 1s 848us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 34/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 35/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 36/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 37/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 38/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 39/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 40/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 41/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 42/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 43/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 44/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 45/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 46/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 47/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 48/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 49/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 50/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 51/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 52/500\n",
            "1149/1149 [==============================] - 1s 815us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 53/500\n",
            "1149/1149 [==============================] - 1s 807us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 54/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 55/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 56/500\n",
            "1149/1149 [==============================] - 1s 839us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 57/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 58/500\n",
            "1149/1149 [==============================] - 1s 790us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 59/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 60/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 61/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 62/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 63/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 64/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 65/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 66/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 67/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 68/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 69/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 70/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 71/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 72/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 73/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 74/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 75/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 76/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 77/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 78/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 79/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 80/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 81/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 82/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 83/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 84/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 85/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 86/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 87/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 88/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 89/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 90/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 91/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 92/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 93/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 94/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 95/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 96/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 97/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 98/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 99/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 100/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 101/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 102/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 103/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 104/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 105/500\n",
            "1149/1149 [==============================] - 1s 841us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 106/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 107/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 108/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 109/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 110/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 111/500\n",
            "1149/1149 [==============================] - 1s 849us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 112/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 113/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 114/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 115/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 116/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 117/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 118/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 119/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 120/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 121/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 122/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 123/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 14.0139 - acc: 0.1305 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 124/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 13.9965 - acc: 0.1314 - val_loss: 13.9718 - val_acc: 0.1332\n",
            "Epoch 125/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 10.3210 - acc: 0.3577 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 126/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 127/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 128/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 129/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 130/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 131/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 132/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 133/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 134/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 135/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 136/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 137/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 138/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 139/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 140/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 141/500\n",
            "1149/1149 [==============================] - 1s 807us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 142/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 143/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 144/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 145/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 146/500\n",
            "1149/1149 [==============================] - 1s 792us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 147/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 148/500\n",
            "1149/1149 [==============================] - 1s 807us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 149/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 150/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 151/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 152/500\n",
            "1149/1149 [==============================] - 1s 840us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 153/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 154/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 155/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 156/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 157/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 158/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 159/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 160/500\n",
            "1149/1149 [==============================] - 1s 841us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 161/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 162/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 163/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 164/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 165/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 166/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 167/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 168/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 169/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 170/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 171/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 172/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 173/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 174/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 175/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 176/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 177/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 178/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 179/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 180/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 181/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 182/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 183/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 184/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 185/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 186/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 187/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 188/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 189/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 190/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 191/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 192/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 193/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 194/500\n",
            "1149/1149 [==============================] - 1s 815us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 195/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 196/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 197/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 198/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 199/500\n",
            "1149/1149 [==============================] - 1s 850us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 200/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 201/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 202/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 203/500\n",
            "1149/1149 [==============================] - 1s 815us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 204/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 205/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 206/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 207/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 208/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 209/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 210/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 211/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 212/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 213/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 214/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 215/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 216/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 217/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 218/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 219/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 220/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 221/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 222/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 223/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 224/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 225/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 226/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 227/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 228/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 229/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 230/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 231/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 232/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 233/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 234/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 235/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 236/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 237/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 238/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 239/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 240/500\n",
            "1149/1149 [==============================] - 1s 797us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 241/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 242/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 243/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 244/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 245/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 246/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 247/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 248/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 249/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 250/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 251/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 252/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 253/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 254/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 255/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 256/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 257/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 258/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 259/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 260/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 261/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 262/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 263/500\n",
            "1149/1149 [==============================] - 1s 787us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 264/500\n",
            "1149/1149 [==============================] - 1s 796us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 265/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 266/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 267/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 268/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 269/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 270/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 271/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 272/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 273/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 274/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 275/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 276/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 277/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 278/500\n",
            "1149/1149 [==============================] - 1s 798us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 279/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 280/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 281/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 282/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 283/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 284/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 285/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 286/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 287/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 288/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 289/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 290/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 291/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 292/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 293/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 294/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 295/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 296/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 297/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 298/500\n",
            "1149/1149 [==============================] - 1s 802us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 299/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 300/500\n",
            "1149/1149 [==============================] - 1s 798us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 301/500\n",
            "1149/1149 [==============================] - 1s 790us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 302/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 303/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 304/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 305/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 306/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 307/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 308/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 309/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 310/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 311/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 312/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 313/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 314/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 315/500\n",
            "1149/1149 [==============================] - 1s 796us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 316/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 317/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 318/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 319/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 320/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 321/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 322/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 323/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 324/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 325/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 326/500\n",
            "1149/1149 [==============================] - 1s 799us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 327/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 328/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 329/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 330/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 331/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 332/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 333/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 334/500\n",
            "1149/1149 [==============================] - 1s 840us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 335/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 336/500\n",
            "1149/1149 [==============================] - 1s 801us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 337/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 338/500\n",
            "1149/1149 [==============================] - 1s 840us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 339/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 340/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 341/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 342/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 343/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 344/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 345/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 346/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 347/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 348/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 349/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 350/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 351/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 352/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 353/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 354/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 355/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 356/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 357/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 358/500\n",
            "1149/1149 [==============================] - 1s 843us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 359/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 360/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 361/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 362/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 363/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 364/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 365/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 366/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 367/500\n",
            "1149/1149 [==============================] - 1s 807us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 368/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 369/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 370/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 371/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 372/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 373/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 374/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 375/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 376/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 377/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 378/500\n",
            "1149/1149 [==============================] - 1s 789us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 379/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 380/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 381/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 382/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 383/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 384/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 385/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 386/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 387/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 388/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 389/500\n",
            "1149/1149 [==============================] - 1s 801us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 390/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 391/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 392/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 393/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 394/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 395/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 396/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 397/500\n",
            "1149/1149 [==============================] - 1s 797us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 398/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 399/500\n",
            "1149/1149 [==============================] - 1s 787us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 400/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 401/500\n",
            "1149/1149 [==============================] - 1s 790us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 402/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 403/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 404/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 405/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 406/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 407/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 408/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 409/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 410/500\n",
            "1149/1149 [==============================] - 1s 812us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 411/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 412/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 413/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 414/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 415/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 416/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 417/500\n",
            "1149/1149 [==============================] - 1s 789us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 418/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 419/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 420/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 421/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 422/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 423/500\n",
            "1149/1149 [==============================] - 1s 802us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 424/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 425/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 426/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 427/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 428/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 429/500\n",
            "1149/1149 [==============================] - 1s 830us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 430/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 431/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 432/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 433/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 434/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 435/500\n",
            "1149/1149 [==============================] - 1s 815us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 436/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 437/500\n",
            "1149/1149 [==============================] - 1s 848us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 438/500\n",
            "1149/1149 [==============================] - 1s 841us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 439/500\n",
            "1149/1149 [==============================] - 1s 801us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 440/500\n",
            "1149/1149 [==============================] - 1s 825us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 441/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 442/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 443/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 444/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 445/500\n",
            "1149/1149 [==============================] - 1s 826us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 446/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 447/500\n",
            "1149/1149 [==============================] - 1s 841us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 448/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 449/500\n",
            "1149/1149 [==============================] - 1s 792us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 450/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 451/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 452/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 453/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 454/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 455/500\n",
            "1149/1149 [==============================] - 1s 833us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 456/500\n",
            "1149/1149 [==============================] - 1s 794us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 457/500\n",
            "1149/1149 [==============================] - 1s 791us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 458/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 459/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 460/500\n",
            "1149/1149 [==============================] - 1s 824us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 461/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 462/500\n",
            "1149/1149 [==============================] - 1s 799us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 463/500\n",
            "1149/1149 [==============================] - 1s 814us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 464/500\n",
            "1149/1149 [==============================] - 1s 811us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 465/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 466/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 467/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 468/500\n",
            "1149/1149 [==============================] - 1s 818us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 469/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 470/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 471/500\n",
            "1149/1149 [==============================] - 1s 813us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 472/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 473/500\n",
            "1149/1149 [==============================] - 1s 803us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 474/500\n",
            "1149/1149 [==============================] - 1s 823us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 475/500\n",
            "1149/1149 [==============================] - 1s 817us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 476/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 477/500\n",
            "1149/1149 [==============================] - 1s 800us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 478/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 479/500\n",
            "1149/1149 [==============================] - 1s 806us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 480/500\n",
            "1149/1149 [==============================] - 1s 819us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 481/500\n",
            "1149/1149 [==============================] - 1s 820us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 482/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 483/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 484/500\n",
            "1149/1149 [==============================] - 1s 810us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 485/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 486/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 487/500\n",
            "1149/1149 [==============================] - 1s 805us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 488/500\n",
            "1149/1149 [==============================] - 1s 821us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 489/500\n",
            "1149/1149 [==============================] - 1s 815us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 490/500\n",
            "1149/1149 [==============================] - 1s 809us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 491/500\n",
            "1149/1149 [==============================] - 1s 840us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 492/500\n",
            "1149/1149 [==============================] - 1s 829us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 493/500\n",
            "1149/1149 [==============================] - 1s 799us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 494/500\n",
            "1149/1149 [==============================] - 1s 822us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 495/500\n",
            "1149/1149 [==============================] - 1s 804us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 496/500\n",
            "1149/1149 [==============================] - 1s 816us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 497/500\n",
            "1149/1149 [==============================] - 1s 827us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 498/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 499/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n",
            "Epoch 500/500\n",
            "1149/1149 [==============================] - 1s 808us/sample - loss: 9.1182 - acc: 0.4343 - val_loss: 9.0901 - val_acc: 0.4360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcVOWd7/HPrxdpNtlsUQEFo4AG\nIsRWNMTRqPEiojgukATXeMO4JKLXGDGJY/Rl7nVucjVxwkCIEhOHIS6EkXEZoriNcW0IsggCelEa\nFRoEBLGF7v7NH+dU2yCn96rT1PN9v14dqs45Vec5lba+/SzneczdERGRcBWkXQAREUmXgkBEJHAK\nAhGRwCkIREQCpyAQEQmcgkBEJHAKApEGmNn9ZnZHE49dY2ant/Z9RHJNQSAiEjgFgYhI4BQEss+L\nm2RuNLPFZvaJmd1nZr3N7Ekz22ZmT5tZj3rHn2Nmy8xsi5k9Z2ZH1ds33MwWxq97ECjZ41xjzGxR\n/NqXzOwrLSzz98xstZl9ZGZzzeyQeLuZ2d1mtsHMPjazJWY2JN432szejMu2zsx+2KIPTGQPCgLJ\nF+cD3wQGAmcDTwI/BkqJfs+vBTCzgcAs4Lp43xPAf5jZfma2H/DvwANAT+Dh+H2JXzscmAH8A9AL\n+C0w18w6NKegZnYq8H+AccDBwLvAn+LdZwB/F19Ht/iYTfG++4B/cPeuwBDgmeacVySJgkDyxT+7\n+3p3Xwf8F/Cqu//N3auAOcDw+LjxwOPu/pS77wJ+CXQEvgacABQDv3L3Xe7+CPB6vXNMBH7r7q+6\ne427/wH4LH5dc0wAZrj7Qnf/DLgZONHM+gO7gK7AYMDcfbm7fxC/bhdwtJnt7+6b3X1hM88rslcK\nAskX6+s9/nQvz7vEjw8h+gscAHevBdYCfeJ963z3mRjfrff4MOCGuFloi5ltAfrFr2uOPcuwneiv\n/j7u/gzwG2AKsMHMppvZ/vGh5wOjgXfN7HkzO7GZ5xXZKwWBhOZ9oi90IGqTJ/oyXwd8APSJt2Uc\nWu/xWuDn7t693k8nd5/VyjJ0JmpqWgfg7ve4+7HA0URNRDfG219397HAgURNWA8187wie6UgkNA8\nBJxlZqeZWTFwA1HzzkvAy0A1cK2ZFZvZecDx9V77O+BKMxsRd+p2NrOzzKxrM8swC7jczIbF/Qv/\nm6gpa42ZHRe/fzHwCVAF1MZ9GBPMrFvcpPUxUNuKz0GkjoJAguLubwEXAf8MbCTqWD7b3Xe6+07g\nPOAy4COi/oQ/13ttOfA9oqabzcDq+NjmluFp4BZgNlEt5EvAt+Ld+xMFzmai5qNNwC/ifRcDa8zs\nY+BKor4GkVYzLUwjIhI21QhERAKnIBARCZyCQEQkcAoCEZHAFaVdgKY44IADvH///mkXQ0Rkn7Jg\nwYKN7l7a2HH7RBD079+f8vLytIshIrJPMbN3Gz9KTUMiIsFTEIiIBE5BICISuH2ij2Bvdu3aRUVF\nBVVVVWkXZZ9XUlJC3759KS4uTrsoIpKCfTYIKioq6Nq1K/3792f3ySKlOdydTZs2UVFRwYABA9Iu\njoikIGtNQ2Y2I15ub+le9t1gZm5mB7T0/auqqujVq5dCoJXMjF69eqlmJRKwbPYR3A+M2nOjmfUj\nWo7vvdaeQCHQNvQ5ioQta01D7v5CvPTenu4GfgQ8mq1zt5VPPqtmW1V12sXIiY+rdrFy/TYG9m7u\n1Poisq/LaR+BmY0lWgrwjcb+CjWziURrxHLooYc2eGxW7PqU/Tatplcg03RvrFrPh9OuZ+Ct89Mu\niojkWM6CwMw6AT8mahZqlLtPB6YDlJWV5f7buLqKYqrZXtCFLh1LvrB7y9at/NvDj3L1/7ykWW87\n+sJL+bd776F7t27Net1lV/8vxvyP07hg7FnNel2T2Ua+WrssO+8tIu1aLmsEXwIGAJnaQF9goZkd\n7+4f5rAczfJRQS+6dO/5he1bttTwL/fP4uof/mS37dXV1RQVJX+sTzz1TMsKsl9n6HwAdO/Xstc3\norrgXUwrH4oEKWdB4O5LiBbdBsDM1gBl7r6xte99238s4833P27t2+zm6AP349YRBZDQgjV58mTe\nfvtthg0bRnFxMSUlJfTo0YMVK1awcuVKzj33XNauXUtVVRWTJk1i4sSJwOfzJm3fvp0zzzyTr3/9\n67z00kv06dOHRx99lI4dOzZatvnz5/PDH/6Q6upqjjvuOKZOnUqHDh2YPHkyc+fOpaioiDPOOINf\n/vKXPPzww9x2220UFhbSrVs3XnjhhYR3NQoVBCJByloQmNks4BTgADOrAG519/uydb5cu/POO1m6\ndCmLFi3iueee46yzzmLp0qV1Y/FnzJhBz549+fTTTznuuOM4//zz6dWr127vsWrVKmbNmsXvfvc7\nxo0bx+zZs7nooosaPG9VVRWXXXYZ8+fPZ+DAgVxyySVMnTqViy++mDlz5rBixQrMjC1btgBw++23\nM2/ePPr06VO3LYkRRn+IiOwum6OGvt3I/v5tda5bz/5yW73V53Z8BFveTaoQfMHxxx+/2w1Z99xz\nD3PmzAFg7dq1rFq16gtBMGDAAIYNGwbAsccey5o1axo9z1tvvcWAAQMYOHAgAJdeeilTpkzh+9//\nPiUlJVxxxRWMGTOGMWPGADBy5Eguu+wyxo0bx3nnndfgexfguLuGk4oERnMNtZHOnTvXPX7uued4\n+umnefnll3njjTcYPnz4Xm/Y6tChQ93jwsJCqqtbPlS1qKiI1157jQsuuIDHHnuMUaOiWzimTZvG\nHXfcwdq1azn22GPZtGnT3t/AoIBaalUpEAnOPjvFRO7s/a/jrl27sm3btr3u27p1Kz169KBTp06s\nWLGCV155pc1KM2jQINasWcPq1as54ogjeOCBBzj55JPZvn07O3bsYPTo0YwcOZLDDz8cgLfffpsR\nI0YwYsQInnzySdauXfuFmknEKDRnZ00thQWFbVZeEWn/FAQt1KtXL0aOHMmQIUPo2LEjvXv3rts3\natQopk2bxlFHHcWgQYM44YQT2uy8JSUl/P73v+fCCy+s6yy+8sor+eijjxg7dixVVVW4O3fddRcA\nN954I6tWrcLdOe200zjmmGMafP9arwUUBCIhMd8HbpgqKyvzPVcoW758OUcddVT2Thr3Eawt6k+/\nA3tk7zztxKLXX2LY42fyyU3r6byX+yZEZN9jZgvcvayx49RHkCgOyGD6TaMLrakJY0oNEfmcmoYa\nldskuOaaa/jrX/+627ZJkyZx+eWX5+T8tTU1OTmPiLQfCoJ2ZsqUKemcOM67GgWBSHDUNCS7iTqL\nRSQkCoJGhNJFYPGV1larRiASGgVBkvY/mCoralxBIBIaBUFjQqkSxFx9BCLBURAkylQJ2iYJunTp\nkrhvzZo1DBkypE3O02J1ncXqIxAJjYJAYnEfQa3uIxAJTX4MH31yMny4pG3f84Aj4NjLEndPnjyZ\nfv36cc011wDws5/9jKKiIp599lk2b97Mrl27uOOOOxg7dmyzTltVVcVVV11FeXk5RUVF3HXXXXzj\nG99g2bJlXH755ezcuZPa2lpmz57NIYccwrhx46ioqKCmpoZbbrmF8ePHt+aqdR+BSIDyIwhSMH78\neK677rq6IHjooYeYN28e1157Lfvvvz8bN27khBNO4JxzzmnWtM5TpkzBzFiyZAkrVqzgjDPOYOXK\nlUybNo1JkyYxYcIEdu7cSU1NDU888QSHHHIIjz/+OBBNdtdaGj4qEp78CIIz72z79/xkI2xdm9hD\nMHz4cDZs2MD7779PZWUlPXr04KCDDuL666/nhRdeoKCggHXr1rF+/XoOOuigJp/2xRdf5Ac/+AEA\ngwcP5rDDDmPlypWceOKJ/PznP6eiooLzzjuPI488kqFDh3LDDTdw0003MWbMGE466aSWX28cVqoR\niIRHfQSNaeCv+QsvvJBHHnmEBx98kPHjxzNz5kwqKytZsGABixYtonfv3ntdh6AlvvOd7zB37lw6\nduzI6NGjeeaZZxg4cCALFy5k6NCh/PSnP+X2229v9XkUBCLhyY8aQVY0fiPB+PHj+d73vsfGjRt5\n/vnneeihhzjwwAMpLi7m2Wef5d133232WU866SRmzpzJqaeeysqVK3nvvfcYNGgQ77zzDocffjjX\nXnst7733HosXL2bw4MH07NmTiy66iO7du3Pvvfe25EKBz8dGqWlIJDwKglb48pe/zLZt2+jTpw8H\nH3wwEyZM4Oyzz2bo0KGUlZUxePDgZr/n1VdfzVVXXcXQoUMpKiri/vvvp0OHDjz00EM88MADFBcX\nc9BBB/HjH/+Y119/nRtvvJGCggKKi4uZOnVqq69JNQKR8Gg9giSfVMLWCtZ1+BJ9eu2fvfO0E4sX\nvs5X5p7OknOfZuiw49Iujoi0gdTXIzCzGWa2wcyW1tv2CzNbYWaLzWyOmXXP1vmlmeK2Id1HIBKe\nbHYW3w+M2mPbU8AQd/8KsBK4OYvnbyNtN8fEkiVLGDZs2G4/I0aMaLP3bwuuO4tFgpO1PgJ3f8HM\n+u+x7S/1nr4CXNDKczRrjH7z3rzt33Lo0KEsWrSo7d+4laLmwXj4qCadEwlOmsNHvws8mbTTzCaa\nWbmZlVdWVn5hf0lJCZs2bSLbfRzZypn2wt3ZtGkTBTWfAeosFglRKqOGzOwnQDUwM+kYd58OTIeo\ns3jP/X379qWiooK9hUSb+GwbfLqZLcWwdX1+L+ZeUlJCyWfrAXDVCESCk/MgMLPLgDHAad6KP+eL\ni4sZMGBAm5XrC16ZBvNu4vajH+cfxw3P3nnaiTUfRk1WteojEAlOToPAzEYBPwJOdvcduTx383n8\nv3neNhSzgqiVsLZWQSASmmwOH50FvAwMMrMKM7sC+A3QFXjKzBaZ2bRsnb/V4spKQUFYQUCtmoZE\nQpPNUUPf3svm+7J1vrYXBUEYMQAFBYUA1CoIRIKjSeeSxDWCur+U85zFQeCaa0gkOGF8y7WCBVIn\nUI1AJFwKgkTxgKZ8v5EgVhDXfLR4vUh4FARJMk1DYeQABYVqGhIJlYIgUeYWhzA+okzTkGv4qEhw\nwviWa4nM8NFAqgR19xGoRiASHAVBYwIJgkyNgBpNQy0SGgVBorBuKCvMdBaraUgkOAqCJJlpkMLI\nAawwurdQncUi4VEQJAq1s1jDR0VCE8a3XEsE1llcWBg3DWkaapHgKAgaE0gQZKaYQH0EIsFRECTK\n1AjC+IgKM30Etdld8U1E2p8wvuVaIqy+4s+nmFDTkEhwFAQJ6kbPBDJ81EzrEYiESkGQwOvmGgoj\nCLBMjUB9BCKhURA0KpCPSOsRiAQrkG+55vPAho/W1QjUNCQSHAVBksCmoc4EQa3WIxAJTjYXr59h\nZhvMbGm9bT3N7CkzWxX/2yNb528tJ2oiCWWpykwQVCsIRIKTzW+5+4FRe2ybDMx39yOB+fHz9im4\nGkHUR1BdrSAQCU3WgsDdXwA+2mPzWOAP8eM/AOdm6/yt5Q61bgH1EUTXWaNpqEWCk+t2j97u/kH8\n+EOgd9KBZjbRzMrNrLyysjI3pavH3XHCuaGsro9AncUiwUmtAdyjYTmJ8xm4+3R3L3P3stLS0hyW\nrK4EQEhNQ+ojEAlVroNgvZkdDBD/uyHH52+yqEYQUNNQfB9BjYJAJDi5DoK5wKXx40uBR3N8/ibL\nBEEwNHxUJFjZHD46C3gZGGRmFWZ2BXAn8E0zWwWcHj9vt5zwbihTjUAkPEXZemN3/3bCrtOydc62\n5F6LY8H1EdRqPQKR4ARyt1QLBDfFRNRHUKvhoyLBURAkiAY1hVQjiC5Uw0dFwqMgSJK5jyCUJDDD\nMdxrqa5R85BISBQECaKbHCykcUO4FVCAU1WtIBAJiYIgSTx8NJQKAYBTQCG1VO1S85BISLI2amjf\nF1hnMVGNwHDe3bSDz1QrEGkXenXej5LiwqyeQ0GQILNSVzgxAFgBYwpf4e17z2F12mUREQA+PPNn\nHPu17I66VxAkiGcaCqpGwLAJdHrnNYYkzgAlIrnmPfbL+jkUBElq4ykmAsqBorP/Hz3TLoSI5Jw6\nixN5WFNMiEiwFAQJMovXKwZEJN8pCJKEdmexiARLfQQJMv2lahoSkXynIEgQ3FxDIhIsNQ0lytxZ\nrCQQkfymIEgS4g1lIhIkBUESD2yFMhEJloIggRPepHMiEiYFQZLM7KNpl0NEJMtSCQIzu97MlpnZ\nUjObZWYlaZSjIR7awjQiEqycB4GZ9QGuBcrcfQhQCHwr1+VoVObOYuWAiOS5tJqGioCOZlYEdALe\nT6kcDYjuI1BnsYjku5wHgbuvA34JvAd8AGx197/seZyZTTSzcjMrr6yszHUxg1yqUkTClEbTUA9g\nLDAAOATobGYX7Xmcu0939zJ3LystLc11MesWry9Qd7qI5Lk0vuZOB/6/u1e6+y7gz8DXUihHgz6f\nfVR1AhHJb2kEwXvACWbWyaIhOacBy1MoRyPCW5hGRMKURh/Bq8AjwEJgSVyG6bkuR6Pi+wjUWSwi\n+S6V2Ufd/Vbg1jTO3VROZtyQiEh+U1doEtUIRCQQCoIkmdlHlQMikueaFARmNsnM9rfIfWa20MzO\nyHbh0qZJ50QkBE2tEXzX3T8GzgB6ABcDd2atVO2A161HoCQQkfzW1CDIfBuOBh5w92UE0I/qrhqB\niOS/pgbBAjP7C1EQzDOzrkBt9orVDmTuLFYSiEiea+rw0SuAYcA77r7DzHoCl2evWOlzzT4qIoFo\nao3gROAtd98Szwv0U2Br9orVHmSGj6ZdDhGR7GpqEEwFdpjZMcANwNvAH7NWqvYgvo8ggK4QEQlc\nU4Og2qO2krHAb9x9CtA1e8VKX+bOYtUIRCTfNbWPYJuZ3Uw0bPQkMysAirNXrHYgs2axOglEJM81\ntUYwHviM6H6CD4G+wC+yVqp2ITMNtYhIfmtSEMRf/jOBbmY2Bqhy9yD6CDR8VETyXVOnmBgHvAZc\nCIwDXjWzC7JZsNRp+KiIBKKpfQQ/AY5z9w0AZlYKPE20rkBeqluzWEEgInmuqX0EBZkQiG1qxmv3\nUXFnsXoJRCTPNbVG8J9mNg+YFT8fDzyRnSK1E3HTkBavF5F816QgcPcbzex8YGS8abq7z8lesdqB\neK4h1QhEJN81ealKd58NzM5iWdoZVx+BiAShwSAws21kBtTvsQtwd98/K6VqBzKdxYUKAhHJcw0G\ngbtnZRoJM+sO3AsMIfrO/a67v5yNc7VYpmlIVQIRyXNNbhpqY78G/tPdLzCz/YBOKZUjWaazWEEg\nInku50FgZt2AvwMuA3D3ncDOXJejcVFXsSadE5F8l8bgyAFAJfB7M/ubmd1rZp33PMjMJppZuZmV\nV1ZW5r6UmmJCRAKRRhAUAV8Fprr7cOATYPKeB7n7dHcvc/ey0tLSXJexbhpqEZF8l0YQVAAV7v5q\n/PwRomBoXzI1ArUNiUiey3kQxDOZrjWzQfGm04A3c12OxmU6i1MuhohIlqU1augHwMx4xNA7wOUp\nlSNZPHxUfQQiku9SCQJ3XwSUpXHupstMOicikt80pVqCz6ehVhSISH5TECSpaxpKuyAiItmlIEii\nO4tFJBAKgkSZO4sVBCKS3xQESeL7CNRbLCL5TkHQAPURiEgIFARJNNeQiARCQZBIncUiEgYFQQJ3\nLVUpImFQECQwMiuUpV0SEZHsUhAkyNxZrKYhEcl3CoIk6iwWkUAoCBJpGmoRCYOCIEk8xYQmnROR\nfKcgSGDxNNQiIvlOQZAgqhAoCEQk/ykIEmnpehEJg4IgkZqGRCQMCoIkrhqBiIQhtSAws0Iz+5uZ\nPZZWGRrmuq1YRIKQZo1gErA8xfM3zFHTkIgEIZUgMLO+wFnAvWmcv2kUAyIShrRqBL8CfgTUJh1g\nZhPNrNzMyisrK3NXsjqOq2lIRAKQ8yAwszHABndf0NBx7j7d3cvcvay0tDRHpfuc6T4CEQlEGjWC\nkcA5ZrYG+BNwqpn9awrlaITrTgIRCULOg8Ddb3b3vu7eH/gW8Iy7X5TrcjQmXo0g7WKIiGSd7iNI\nohwQkUAUpXlyd38OeC7NMiQxJYGIBEI1gkQKAhEJg4IgievOYhEJg4KgAbqlTERCoCBIpMGjIhIG\nBUESNQ2JSCAUBAk0akhEQqEgaJCCQETyn4IggWthGhEJhIIggdX9j4hIflMQJFIfgYiEQUGQxBUE\nIhIGBUEC08I0IhIIBUGCqKtYQSAi+U9BkMA0wYSIBEJB0BA1DYlIABQECUydxSISCAVBInUWi0gY\nFAQNUhCISP5TECRSZ7GIhCHnQWBm/czsWTN708yWmdmkXJehKUzTUItIINJYvL4auMHdF5pZV2CB\nmT3l7m+mUJYGuOoEIhKEnNcI3P0Dd18YP94GLAf65LocIiISSbWPwMz6A8OBV9MsRyJTF4qI5L/U\nvunMrAswG7jO3T/ey/6JZlZuZuWVlZW5Lx+uLgIRCUIqQWBmxUQhMNPd/7y3Y9x9uruXuXtZaWlp\nbgsYFQANHxWREKQxasiA+4Dl7n5Xrs/fVIajNcpEJARp1AhGAhcDp5rZovhndArlaJCDho+KSBBy\nPnzU3V9kH2hzMa1QJiKB0LCYBphqBCISAAVBAs0+KiKhUBAkcuWAiARBQdAgJYGI5D8FQQJDk86J\nSBgUBA0w1QhEJAAKggSG48oBEQmAgiBBdB+BPh4RyX95/U23q6aWdyq3t/j16iIQkRDkdRBMnr2E\ni6c9T21tC2YNclcfgYgEIa+D4MpPpzNz1/Us//ALs1w3Sn0EIhKKvA6Cg/ofTf+C9SxavKiF75DX\nH4+ICJDn33Rdj/4mAPbOc81+rWnFYhEJRF4HAQccyScFXem6+c2WvV5LVYpIAPL7m86MbR370u2z\n96naVdO8l2qpShEJRH4HAeA9+tOP9Sx7f2vzX6vGIREJQN4HQfc+R9LHNvLyqg3Nep3mGhKRUOR9\nEHTsfST7WQ1b3ngM96bfTxB1FisIRCT/5X0QcPS5bO5yBJdsncY9j5fjn2yEjz/Y7ZC1H+3gu7/+\nd9a+8dzur1UOiEgAcr5mcc6V7E+Xk39Aj8cnMan8dHaVF1JMDVv3682Wi5+m18N/T7+PVzMDYA48\nvWM+pxctois76FTT/BvRRET2NanUCMxslJm9ZWarzWxyts9XPPTv8e6HRY+JRg9127meNdMn0OXj\n1bsdu/2JW/ho3v8F4MNOA7NdNBGR1OW8RmBmhcAU4JtABfC6mc119xYO9m+Ckm7YdYvh5X+BTzfD\nrh3ULPgjJ+9cDEAVHai65g2K/uufOHfx76Ea7q4+n8EjrslakURE2os0moaOB1a7+zsAZvYnYCyQ\nvSDIOPHquoeFp9xM9YI/Utu5lJJjLqQEYPRt0Odotq9bxnlfvZbD+h+c9SKJiKQtjSDoA6yt97wC\nGLHnQWY2EZgIcOihh7Z9KTp0oehrV+++raQbjJhIF6BL259RRKRdarejhtx9uruXuXtZaWlp2sUR\nEclbaQTBOqBfved9420iIpKCNILgdeBIMxtgZvsB3wLmplAOEREhhT4Cd682s+8D84BCYIa7L8t1\nOUREJJLKDWXu/gTwRBrnFhGR3bXbzmIREckNBYGISOAUBCIigbPmTM2cFjOrBN5t4csPADa2YXH2\nBbrmMOiaw9Caaz7M3Ru9EWufCILWMLNydy9Luxy5pGsOg645DLm4ZjUNiYgETkEgIhK4EIJgetoF\nSIGuOQy65jBk/Zrzvo9AREQaFkKNQEREGqAgEBEJXF4HQa7XRs4VM5thZhvMbGm9bT3N7CkzWxX/\n2yPebmZ2T/wZLDazr6ZX8pYxs35m9qyZvWlmy8xsUrw9b68ZwMxKzOw1M3sjvu7b4u0DzOzV+Poe\njGfxxcw6xM9Xx/v7p1n+ljKzQjP7m5k9Fj/P6+sFMLM1ZrbEzBaZWXm8LWe/33kbBPXWRj4TOBr4\ntpkdnW6p2sz9wKg9tk0G5rv7kcD8+DlE139k/DMRmJqjMralauAGdz8aOAG4Jv7/Mp+vGeAz4FR3\nPwYYBowysxOAfwLudvcjgM3AFfHxVwCb4+13x8ftiyYBy+s9z/frzfiGuw+rd89A7n6/3T0vf4AT\ngXn1nt8M3Jx2udrw+voDS+s9fws4OH58MPBW/Pi3wLf3dty++gM8CnwzsGvuBCwkWtZ1I1AUb6/7\nPSea2v3E+HFRfJylXfZmXmff+EvvVOAxwPL5eutd9xrggD225ez3O29rBOx9beQ+KZUlF3q7+wfx\n4w+B3vHjvPoc4ur/cOBVArjmuJlkEbABeAp4G9ji7tXxIfWvre664/1bgV65LXGr/Qr4EVAbP+9F\nfl9vhgN/MbMF8XrtkMPf71TWI5Dscnc3s7wbF2xmXYDZwHXu/rGZ1e3L12t29xpgmJl1B+YAg1Mu\nUtaY2Rhgg7svMLNT0i5Pjn3d3deZ2YHAU2a2ov7ObP9+53ONILS1kdeb2cEA8b8b4u158TmYWTFR\nCMx09z/Hm/P6mutz9y3As0RNI93NLPNHXP1rq7vueH83YFOOi9oaI4FzzGwN8Cei5qFfk7/XW8fd\n18X/biAK/OPJ4e93PgdBaGsjzwUujR9fStSOntl+STzS4ARga73q5j7Boj/97wOWu/td9Xbl7TUD\nmFlpXBPAzDoS9YssJwqEC+LD9rzuzOdxAfCMx43I+wJ3v9nd+7p7f6L/Xp9x9wnk6fVmmFlnM+ua\neQycASwll7/faXeSZLkDZjSwkqhd9Sdpl6cNr2sW8AGwi6h98AqittH5wCrgaaBnfKwRjZ56G1gC\nlKVd/hZc79eJ2lAXA4vin9H5fM3xdXwF+Ft83UuBf4y3Hw68BqwGHgY6xNtL4uer4/2Hp30Nrbj2\nU4DHQrje+PreiH+WZb6rcvn7rSkmREQCl89NQyIi0gQKAhGRwCkIREQCpyAQEQmcgkBEJHAKApEs\nM7NTMjNpirRHCgIRkcApCERiZnZRPP//IjP7bTzh23YzuzteD2C+mZXGxw4zs1fi+eDn1Jsr/ggz\nezpeQ2ChmX0pfvsuZvaIma0ws5lWf6IkkZQpCEQAMzsKGA+MdPdhQA0wAegMlLv7l4HngVvjl/wR\nuMndv0J0d2dm+0xgikdrCHzsagfCAAABIElEQVSN6A5wiGZMvY5obYzDiebVEWkXNPuoSOQ04Fjg\n9fiP9Y5Ek3zVAg/Gx/wr8Gcz6wZ0d/fn4+1/AB6O54vp4+5zANy9CiB+v9fcvSJ+vohoPYkXs39Z\nIo1TEIhEDPiDu9+820azW/Y4rqVzsnxW73EN+m9P2hE1DYlE5gMXxPPBZ9aLPYzov5HMzJffAV50\n963AZjM7Kd5+MfC8u28DKszs3Pg9OphZp5xehUgL6K8SEcDd3zSznxKtElVANLPrNcAnwPHxvg1E\n/QgQTQs8Lf6ifwe4PN5+MfBbM7s9fo8Lc3gZIi2i2UdFGmBm2929S9rlEMkmNQ2JiARONQIRkcCp\nRiAiEjgFgYhI4BQEIiKBUxCIiAROQSAiErj/BjpQy8pv8xzRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "383/383 [==============================] - 1s 3ms/sample - loss: 9.0901 - acc: 0.4360\n",
            "\n",
            "Test result: 43.603 loss: 9.090\n",
            "[9.09010085082241, 0.43603134]\n",
            "[242, 96, 315, 115, 260, 556, 284, 652, 363, 814, 118, 524, 433, 339, 434, 272, 28, 445, 698, 1054, 633, 772, 580, 417, 231, 79, 235, 169, 318, 7, 332, 644, 355, 86, 99, 824, 709, 727, 391, 908, 703, 43, 658, 680, 215, 1033, 638, 1079, 269, 63, 740, 258, 457, 1036, 176, 171, 730, 1044, 960, 91, 1014, 549, 309, 420, 1116, 308, 82, 428, 77, 546, 468, 883, 783, 968, 422, 155, 682, 606, 40, 425, 543, 808, 334, 504, 536, 802, 18, 253, 752, 498, 929, 283, 376, 328, 412, 780, 27, 201, 1037, 962, 305, 285, 859, 641, 884, 1100, 892, 932, 237, 399, 233, 767, 175, 10, 1129, 500, 1025, 1107, 1114, 818, 1072, 766, 162, 687, 639, 451, 1134, 961, 891, 834, 851, 436, 689, 805, 866, 1005, 94, 852, 393, 67, 371, 710, 1142, 1083, 938, 729, 73, 656, 950, 815, 54, 470, 980, 1135, 953, 1098, 967, 218, 538, 595, 888, 496, 1080, 914, 898, 1020, 845, 762, 191, 982, 47, 1126, 986, 746, 530, 35, 857, 711, 820, 676, 781, 981, 831, 107, 145, 754, 427, 572, 700, 1009, 261, 792, 165, 435, 1048, 45, 78, 511, 1028, 121, 513, 405, 1024, 901, 939, 696, 312, 790, 577, 653, 1102, 12, 98, 521, 1104, 313, 552, 172, 185, 486, 60, 509, 303, 586, 15, 736, 72, 750, 22, 390, 192, 379, 429, 144, 1143, 462, 678, 1002, 317, 630, 326, 817, 679, 8, 46, 525, 227, 881, 856, 59, 623, 742, 402, 1140, 401, 471, 765, 970, 941, 758, 1032, 252, 182, 635, 775, 579, 216, 830, 640, 523, 646, 1064, 949, 302, 167, 1039, 89, 226, 671, 163, 728, 514, 1108, 1112, 683, 208, 1057, 522, 323, 844, 301, 338, 673, 426, 14, 126, 136, 1022, 1074, 494, 578, 739, 129, 564, 454, 560, 789, 608, 1138, 836, 706, 557, 990, 988, 195, 1053, 675, 288, 199, 5, 721, 734, 839, 667, 29, 672, 900, 649, 225, 975, 664, 212, 450, 600, 915, 763, 44, 197, 1063, 36, 80, 878, 306, 352, 800, 141, 277, 499, 576, 947, 1011, 465, 642, 1096, 544, 24, 456, 518, 259, 478, 614, 441, 571, 349, 385, 184, 904, 276, 945, 58, 1088, 1127, 1000, 519, 411, 1034, 542, 295, 1110, 533, 1021, 17, 745, 1092, 813, 13, 321, 491, 1068, 551, 460, 251, 1006, 713, 1149, 751, 1123, 69, 965, 942, 704, 173, 694, 105, 370, 607, 293, 492, 418, 374, 992, 690, 702, 482, 931, 254, 1038, 773, 469, 206, 849, 554, 733, 416, 66, 624, 166, 985, 613, 1111, 620, 324, 1090, 663, 930, 605, 872, 1119, 677, 684, 593, 539, 241, 347, 369, 344, 959, 629, 298, 726, 599, 316, 64, 693, 660, 691, 760, 612, 84, 240, 95, 357, 360, 840, 489, 951, 250, 159, 737, 1062, 160, 843, 784, 1085, 974, 944, 893, 25, 545, 846, 590, 862, 812, 362, 906, 987, 52, 976, 279, 1139, 310, 238, 220, 1109, 1141, 9, 214, 566, 351, 928, 410, 1124, 407, 1076, 190, 1084, 822, 618, 731, 761, 943, 404, 1045, 759, 913, 865, 1023, 716, 101, 377, 437, 93, 670, 969, 256, 459, 148, 1121, 120, 70, 281, 826, 267, 627, 526, 146, 777, 979, 380, 1148, 83, 507, 273, 529, 373, 149, 239, 534, 924, 1106, 353, 100, 1007, 911, 188, 810, 200, 648, 383, 674, 714, 178, 559, 473, 56, 875, 948, 221, 274, 1137, 234, 193, 452, 885, 495, 978, 535, 325, 1056, 453, 408, 591, 963, 661, 257, 705, 396, 204, 346, 1094, 3, 594, 643, 113, 1086, 278, 909, 637, 31, 1041, 569, 388, 194, 1073, 1043, 589, 550, 458, 776, 1016, 291, 565, 71, 547, 1029, 515, 659, 853, 512, 398, 744, 528, 827, 207, 23, 619, 87, 785, 488, 615, 804, 686, 85, 158, 282, 747, 484, 1047, 400, 104, 92, 997, 345, 1105, 842, 481, 392, 854, 863, 516, 122, 187, 50, 610, 290, 419, 359, 994, 1018, 855, 53, 1042, 42, 860, 636, 1118, 907, 111, 841, 55, 749, 867, 647, 541, 112, 109, 442, 6, 570, 387, 382, 879, 510, 1070, 222, 1058, 584, 130, 874, 695, 75, 1115, 156, 1120, 320, 935, 771, 574, 779, 583, 952, 791, 403, 341, 266, 139, 617, 520, 896, 271, 732, 406, 558, 461, 717, 1132, 738, 1065, 770, 1089, 114, 337, 1059, 625, 322, 555, 333, 413, 133, 764, 1026, 41, 864, 384, 343, 1031, 74, 701, 174, 966, 223, 787, 476, 654, 902, 247, 668, 837, 181, 21, 793, 137, 933, 76, 119, 30, 838, 786, 116, 655, 164, 925, 38, 485, 621, 517, 681, 768, 125, 472, 921, 1078, 372, 300, 423, 972, 723, 897, 850, 871, 1125, 168, 151, 795, 236, 801, 748, 1103, 327, 1136, 350, 912, 934, 889, 877, 0, 958, 886, 993, 1069, 255, 103, 1061, 299, 708, 463, 597, 20, 249, 622, 479, 330, 62, 102, 955, 567, 1144, 152, 1113, 984, 131, 585, 33, 870, 132, 973, 1131, 719, 910, 645, 1010, 741, 211, 110, 697, 563, 936, 348, 368, 490, 828, 905, 232, 946, 531, 311, 601, 127, 720, 202, 847, 725, 210, 124, 1012, 1082, 475, 2, 631, 474, 356, 503, 354, 394, 440, 39, 180, 587, 1117, 32, 1001, 157, 268, 832, 263, 106, 395, 650, 209, 297, 444, 999, 537, 424, 548, 688, 161, 890, 823, 19, 361, 331, 464, 1093, 882, 432, 797, 198, 753, 665, 366, 956, 923, 378, 1060, 262, 575, 991, 448, 189, 477, 340, 540, 611, 147, 1122, 1, 604, 1027, 873, 669, 88, 142, 920, 774, 430, 1019, 493, 940, 1071, 917, 397, 264, 1051, 582, 1013, 743, 205, 1055, 1066, 811, 443, 287, 286, 505, 81, 1052, 480, 265, 735, 1095, 16, 117, 1099, 1091, 918, 245, 829, 788, 937, 364, 1049, 389, 438, 803, 68, 926, 186, 170, 957, 876, 998, 778, 699, 280, 592, 634, 927, 506, 598, 903, 616, 228, 562, 466, 319, 138, 1017, 123, 467, 217, 809, 983, 869, 1130, 37, 48, 755, 977, 365, 848, 487, 203, 835, 229, 1146, 179, 381, 568, 561, 899, 183, 196, 367, 244, 219, 51, 342, 919, 246, 57, 527, 989, 386, 154, 782, 718, 651, 335, 177, 588, 858, 581, 825, 1097, 553, 446, 657, 996, 916, 1050, 483, 294, 833, 1046, 134, 819, 150, 90, 662, 806, 414, 887, 447, 821, 296, 685, 336, 415, 11, 135, 224, 1067, 1133, 153, 1145, 1008, 230, 1128, 798, 1087, 724, 1081, 502, 314, 894, 757, 49, 816, 304, 573, 275, 143, 1040, 34, 421, 715, 1077, 140, 602, 794, 243, 1035, 431, 964, 1004, 603, 292, 1075, 895, 97, 666, 609, 722, 807, 799, 289, 497, 692, 61, 1015, 880, 108, 270, 248, 128, 861, 626, 439, 329, 922, 4, 1147, 712, 796, 707, 307, 532, 628, 358, 971, 632, 375, 1003, 756, 501, 868, 1101, 954, 1030, 995, 769, 409, 213, 596, 449, 65, 455, 26]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 6)       456       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 58, 58, 16)        2416      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 29, 29, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 13456)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 120)               1614840   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 84)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 255       \n",
            "=================================================================\n",
            "Total params: 1,628,131\n",
            "Trainable params: 1,628,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1149 samples, validate on 383 samples\n",
            "Epoch 1/500\n",
            "1149/1149 [==============================] - 1s 1ms/sample - loss: 0.9783 - acc: 0.4961 - val_loss: 0.8512 - val_acc: 0.4334\n",
            "Epoch 2/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.6923 - acc: 0.6954 - val_loss: 0.4172 - val_acc: 0.8773\n",
            "Epoch 3/500\n",
            "1149/1149 [==============================] - 1s 850us/sample - loss: 0.4182 - acc: 0.8390 - val_loss: 0.3692 - val_acc: 0.8172\n",
            "Epoch 4/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.3441 - acc: 0.8590 - val_loss: 0.3240 - val_acc: 0.8198\n",
            "Epoch 5/500\n",
            "1149/1149 [==============================] - 1s 887us/sample - loss: 0.2683 - acc: 0.8747 - val_loss: 0.1393 - val_acc: 0.9713\n",
            "Epoch 6/500\n",
            "1149/1149 [==============================] - 1s 905us/sample - loss: 0.2511 - acc: 0.8886 - val_loss: 0.2715 - val_acc: 0.8433\n",
            "Epoch 7/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.2870 - acc: 0.8634 - val_loss: 0.1923 - val_acc: 0.8773\n",
            "Epoch 8/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.2206 - acc: 0.8956 - val_loss: 0.1078 - val_acc: 0.9661\n",
            "Epoch 9/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.1737 - acc: 0.9269 - val_loss: 0.2829 - val_acc: 0.8747\n",
            "Epoch 10/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.2060 - acc: 0.9051 - val_loss: 0.1950 - val_acc: 0.9008\n",
            "Epoch 11/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.1422 - acc: 0.9408 - val_loss: 0.0657 - val_acc: 0.9843\n",
            "Epoch 12/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0938 - acc: 0.9704 - val_loss: 0.3743 - val_acc: 0.8538\n",
            "Epoch 13/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 0.4470 - acc: 0.8486 - val_loss: 0.1774 - val_acc: 0.9791\n",
            "Epoch 14/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.1920 - acc: 0.9243 - val_loss: 0.1059 - val_acc: 0.9713\n",
            "Epoch 15/500\n",
            "1149/1149 [==============================] - 1s 856us/sample - loss: 0.2585 - acc: 0.8903 - val_loss: 0.3196 - val_acc: 0.8590\n",
            "Epoch 16/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.2307 - acc: 0.8921 - val_loss: 0.2446 - val_acc: 0.8773\n",
            "Epoch 17/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.1924 - acc: 0.9104 - val_loss: 0.0874 - val_acc: 0.9765\n",
            "Epoch 18/500\n",
            "1149/1149 [==============================] - 1s 855us/sample - loss: 0.1228 - acc: 0.9478 - val_loss: 0.0635 - val_acc: 0.9869\n",
            "Epoch 19/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.1047 - acc: 0.9547 - val_loss: 0.0732 - val_acc: 0.9687\n",
            "Epoch 20/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.1778 - acc: 0.9208 - val_loss: 0.1521 - val_acc: 0.9217\n",
            "Epoch 21/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 0.1281 - acc: 0.9460 - val_loss: 0.0999 - val_acc: 0.9530\n",
            "Epoch 22/500\n",
            "1149/1149 [==============================] - 1s 831us/sample - loss: 0.1404 - acc: 0.9382 - val_loss: 0.1160 - val_acc: 0.9399\n",
            "Epoch 23/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.1086 - acc: 0.9626 - val_loss: 0.0470 - val_acc: 0.9896\n",
            "Epoch 24/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0967 - acc: 0.9600 - val_loss: 0.0872 - val_acc: 0.9582\n",
            "Epoch 25/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0901 - acc: 0.9652 - val_loss: 0.0419 - val_acc: 0.9922\n",
            "Epoch 26/500\n",
            "1149/1149 [==============================] - 1s 840us/sample - loss: 0.0835 - acc: 0.9643 - val_loss: 0.0499 - val_acc: 0.9843\n",
            "Epoch 27/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.1034 - acc: 0.9617 - val_loss: 0.0849 - val_acc: 0.9608\n",
            "Epoch 28/500\n",
            "1149/1149 [==============================] - 1s 883us/sample - loss: 0.0757 - acc: 0.9739 - val_loss: 0.0590 - val_acc: 0.9687\n",
            "Epoch 29/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0632 - acc: 0.9730 - val_loss: 0.0675 - val_acc: 0.9661\n",
            "Epoch 30/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.1382 - acc: 0.9356 - val_loss: 0.0426 - val_acc: 0.9948\n",
            "Epoch 31/500\n",
            "1149/1149 [==============================] - 1s 897us/sample - loss: 0.1112 - acc: 0.9547 - val_loss: 0.1467 - val_acc: 0.9269\n",
            "Epoch 32/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.3449 - acc: 0.8599 - val_loss: 0.1543 - val_acc: 0.9634\n",
            "Epoch 33/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 0.2299 - acc: 0.8938 - val_loss: 0.1223 - val_acc: 0.9661\n",
            "Epoch 34/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.1007 - acc: 0.9687 - val_loss: 0.1800 - val_acc: 0.9269\n",
            "Epoch 35/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.1381 - acc: 0.9417 - val_loss: 0.0561 - val_acc: 0.9843\n",
            "Epoch 36/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0629 - acc: 0.9817 - val_loss: 0.0250 - val_acc: 0.9922\n",
            "Epoch 37/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.1387 - acc: 0.9539 - val_loss: 0.2726 - val_acc: 0.8930\n",
            "Epoch 38/500\n",
            "1149/1149 [==============================] - 1s 849us/sample - loss: 0.2022 - acc: 0.9130 - val_loss: 0.1128 - val_acc: 0.9661\n",
            "Epoch 39/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.1098 - acc: 0.9600 - val_loss: 0.1811 - val_acc: 0.9138\n",
            "Epoch 40/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 0.1533 - acc: 0.9452 - val_loss: 0.0810 - val_acc: 0.9634\n",
            "Epoch 41/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0692 - acc: 0.9782 - val_loss: 0.0369 - val_acc: 0.9896\n",
            "Epoch 42/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0522 - acc: 0.9791 - val_loss: 0.0207 - val_acc: 0.9974\n",
            "Epoch 43/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0790 - acc: 0.9704 - val_loss: 0.0527 - val_acc: 0.9817\n",
            "Epoch 44/500\n",
            "1149/1149 [==============================] - 1s 860us/sample - loss: 0.0758 - acc: 0.9678 - val_loss: 0.0586 - val_acc: 0.9817\n",
            "Epoch 45/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.1452 - acc: 0.9434 - val_loss: 0.0469 - val_acc: 0.9896\n",
            "Epoch 46/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0567 - acc: 0.9869 - val_loss: 0.0397 - val_acc: 0.9817\n",
            "Epoch 47/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0549 - acc: 0.9782 - val_loss: 0.0241 - val_acc: 0.9922\n",
            "Epoch 48/500\n",
            "1149/1149 [==============================] - 1s 835us/sample - loss: 0.0577 - acc: 0.9765 - val_loss: 0.0207 - val_acc: 0.9948\n",
            "Epoch 49/500\n",
            "1149/1149 [==============================] - 1s 857us/sample - loss: 0.0644 - acc: 0.9713 - val_loss: 0.2896 - val_acc: 0.8956\n",
            "Epoch 50/500\n",
            "1149/1149 [==============================] - 1s 850us/sample - loss: 0.1410 - acc: 0.9574 - val_loss: 0.0559 - val_acc: 0.9869\n",
            "Epoch 51/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0484 - acc: 0.9869 - val_loss: 0.0184 - val_acc: 0.9922\n",
            "Epoch 52/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0285 - acc: 0.9913 - val_loss: 0.0147 - val_acc: 0.9974\n",
            "Epoch 53/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.0445 - acc: 0.9835 - val_loss: 0.0315 - val_acc: 0.9817\n",
            "Epoch 54/500\n",
            "1149/1149 [==============================] - 1s 855us/sample - loss: 0.0452 - acc: 0.9852 - val_loss: 0.0313 - val_acc: 0.9843\n",
            "Epoch 55/500\n",
            "1149/1149 [==============================] - 1s 839us/sample - loss: 0.0348 - acc: 0.9869 - val_loss: 0.0216 - val_acc: 0.9922\n",
            "Epoch 56/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0151 - val_acc: 0.9948\n",
            "Epoch 57/500\n",
            "1149/1149 [==============================] - 1s 883us/sample - loss: 0.0467 - acc: 0.9800 - val_loss: 0.0125 - val_acc: 0.9974\n",
            "Epoch 58/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.0377 - acc: 0.9878 - val_loss: 0.0204 - val_acc: 0.9974\n",
            "Epoch 59/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0370 - acc: 0.9878 - val_loss: 0.0404 - val_acc: 0.9765\n",
            "Epoch 60/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0272 - acc: 0.9896 - val_loss: 0.0168 - val_acc: 0.9948\n",
            "Epoch 61/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.0515 - acc: 0.9782 - val_loss: 0.2449 - val_acc: 0.9138\n",
            "Epoch 62/500\n",
            "1149/1149 [==============================] - 1s 860us/sample - loss: 0.2760 - acc: 0.9164 - val_loss: 1.0826 - val_acc: 0.6397\n",
            "Epoch 63/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.3643 - acc: 0.8616 - val_loss: 0.1516 - val_acc: 0.9582\n",
            "Epoch 64/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.1416 - acc: 0.9469 - val_loss: 0.0657 - val_acc: 0.9791\n",
            "Epoch 65/500\n",
            "1149/1149 [==============================] - 1s 877us/sample - loss: 0.1071 - acc: 0.9574 - val_loss: 0.2274 - val_acc: 0.8877\n",
            "Epoch 66/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.1153 - acc: 0.9469 - val_loss: 0.0597 - val_acc: 0.9896\n",
            "Epoch 67/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.1300 - acc: 0.9495 - val_loss: 0.1341 - val_acc: 0.9426\n",
            "Epoch 68/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0962 - acc: 0.9643 - val_loss: 0.0512 - val_acc: 0.9765\n",
            "Epoch 69/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0838 - acc: 0.9721 - val_loss: 0.0601 - val_acc: 0.9817\n",
            "Epoch 70/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.0387 - acc: 0.9896 - val_loss: 0.0449 - val_acc: 0.9765\n",
            "Epoch 71/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.0511 - acc: 0.9843 - val_loss: 0.1050 - val_acc: 0.9556\n",
            "Epoch 72/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.0769 - acc: 0.9713 - val_loss: 0.0229 - val_acc: 0.9974\n",
            "Epoch 73/500\n",
            "1149/1149 [==============================] - 1s 841us/sample - loss: 0.0478 - acc: 0.9852 - val_loss: 0.0208 - val_acc: 0.9974\n",
            "Epoch 74/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 0.0449 - acc: 0.9843 - val_loss: 0.0259 - val_acc: 0.9869\n",
            "Epoch 75/500\n",
            "1149/1149 [==============================] - 1s 828us/sample - loss: 0.0238 - acc: 0.9956 - val_loss: 0.0123 - val_acc: 0.9974\n",
            "Epoch 76/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0196 - acc: 0.9956 - val_loss: 0.0080 - val_acc: 0.9974\n",
            "Epoch 77/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0421 - acc: 0.9878 - val_loss: 0.0189 - val_acc: 0.9922\n",
            "Epoch 78/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0253 - acc: 0.9913 - val_loss: 0.0290 - val_acc: 0.9869\n",
            "Epoch 79/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0265 - acc: 0.9922 - val_loss: 0.0369 - val_acc: 0.9765\n",
            "Epoch 80/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0548 - acc: 0.9800 - val_loss: 0.0570 - val_acc: 0.9765\n",
            "Epoch 81/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.1055 - acc: 0.9678 - val_loss: 0.0283 - val_acc: 0.9948\n",
            "Epoch 82/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0346 - acc: 0.9887 - val_loss: 0.0105 - val_acc: 1.0000\n",
            "Epoch 83/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0444 - val_acc: 0.9791\n",
            "Epoch 84/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0368 - acc: 0.9861 - val_loss: 0.0984 - val_acc: 0.9530\n",
            "Epoch 85/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0365 - acc: 0.9887 - val_loss: 0.0162 - val_acc: 0.9974\n",
            "Epoch 86/500\n",
            "1149/1149 [==============================] - 1s 890us/sample - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0091 - val_acc: 1.0000\n",
            "Epoch 87/500\n",
            "1149/1149 [==============================] - 1s 886us/sample - loss: 0.0127 - acc: 0.9974 - val_loss: 0.0061 - val_acc: 0.9974\n",
            "Epoch 88/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0128 - acc: 0.9956 - val_loss: 0.0087 - val_acc: 0.9974\n",
            "Epoch 89/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0108 - acc: 0.9974 - val_loss: 0.0055 - val_acc: 1.0000\n",
            "Epoch 90/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0096 - acc: 0.9974 - val_loss: 0.0055 - val_acc: 0.9974\n",
            "Epoch 91/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0140 - acc: 0.9948 - val_loss: 0.0078 - val_acc: 1.0000\n",
            "Epoch 92/500\n",
            "1149/1149 [==============================] - 1s 888us/sample - loss: 0.0815 - acc: 0.9695 - val_loss: 0.1456 - val_acc: 0.9530\n",
            "Epoch 93/500\n",
            "1149/1149 [==============================] - 1s 859us/sample - loss: 0.1351 - acc: 0.9521 - val_loss: 0.0463 - val_acc: 0.9817\n",
            "Epoch 94/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0414 - acc: 0.9887 - val_loss: 0.0187 - val_acc: 0.9922\n",
            "Epoch 95/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0255 - acc: 0.9913 - val_loss: 0.0494 - val_acc: 0.9765\n",
            "Epoch 96/500\n",
            "1149/1149 [==============================] - 1s 888us/sample - loss: 0.0733 - acc: 0.9748 - val_loss: 0.0355 - val_acc: 0.9869\n",
            "Epoch 97/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0883 - acc: 0.9678 - val_loss: 0.0314 - val_acc: 0.9922\n",
            "Epoch 98/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0666 - acc: 0.9791 - val_loss: 0.0334 - val_acc: 0.9869\n",
            "Epoch 99/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0371 - acc: 0.9852 - val_loss: 0.1696 - val_acc: 0.9504\n",
            "Epoch 100/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.1100 - acc: 0.9669 - val_loss: 0.0593 - val_acc: 0.9843\n",
            "Epoch 101/500\n",
            "1149/1149 [==============================] - 1s 893us/sample - loss: 0.0469 - acc: 0.9826 - val_loss: 0.0126 - val_acc: 0.9948\n",
            "Epoch 102/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0282 - acc: 0.9904 - val_loss: 0.0079 - val_acc: 1.0000\n",
            "Epoch 103/500\n",
            "1149/1149 [==============================] - 1s 848us/sample - loss: 0.1004 - acc: 0.9695 - val_loss: 0.0142 - val_acc: 1.0000\n",
            "Epoch 104/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0492 - acc: 0.9861 - val_loss: 0.0149 - val_acc: 1.0000\n",
            "Epoch 105/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0278 - acc: 0.9896 - val_loss: 0.0052 - val_acc: 1.0000\n",
            "Epoch 106/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 0.0084 - acc: 0.9983 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 107/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0141 - acc: 0.9948 - val_loss: 0.0085 - val_acc: 0.9948\n",
            "Epoch 108/500\n",
            "1149/1149 [==============================] - 1s 856us/sample - loss: 0.0178 - acc: 0.9939 - val_loss: 0.0682 - val_acc: 0.9687\n",
            "Epoch 109/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0333 - acc: 0.9896 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 110/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0221 - acc: 0.9939 - val_loss: 0.0067 - val_acc: 1.0000\n",
            "Epoch 111/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0117 - acc: 0.9974 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 112/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
            "Epoch 113/500\n",
            "1149/1149 [==============================] - 1s 897us/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 114/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 115/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 116/500\n",
            "1149/1149 [==============================] - 1s 903us/sample - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 117/500\n",
            "1149/1149 [==============================] - 1s 892us/sample - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0606 - val_acc: 0.9765\n",
            "Epoch 118/500\n",
            "1149/1149 [==============================] - 1s 884us/sample - loss: 0.2179 - acc: 0.9304 - val_loss: 0.1078 - val_acc: 0.9556\n",
            "Epoch 119/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0874 - acc: 0.9713 - val_loss: 0.0334 - val_acc: 0.9948\n",
            "Epoch 120/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0393 - acc: 0.9896 - val_loss: 0.0239 - val_acc: 0.9948\n",
            "Epoch 121/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0487 - acc: 0.9835 - val_loss: 0.0087 - val_acc: 1.0000\n",
            "Epoch 122/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0217 - acc: 0.9939 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 123/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0458 - acc: 0.9826 - val_loss: 0.0311 - val_acc: 0.9843\n",
            "Epoch 124/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0207 - acc: 0.9939 - val_loss: 0.0048 - val_acc: 1.0000\n",
            "Epoch 125/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 0.0052 - val_acc: 0.9974\n",
            "Epoch 126/500\n",
            "1149/1149 [==============================] - 1s 913us/sample - loss: 0.0059 - acc: 0.9991 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 127/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0052 - acc: 0.9991 - val_loss: 9.0349e-04 - val_acc: 1.0000\n",
            "Epoch 128/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 129/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0050 - acc: 0.9974 - val_loss: 0.0110 - val_acc: 0.9922\n",
            "Epoch 130/500\n",
            "1149/1149 [==============================] - 1s 884us/sample - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 131/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0512 - acc: 0.9791 - val_loss: 0.0038 - val_acc: 1.0000\n",
            "Epoch 132/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0774 - acc: 0.9721 - val_loss: 0.0375 - val_acc: 0.9817\n",
            "Epoch 133/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0302 - acc: 0.9913 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 134/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0119 - acc: 0.9974 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 135/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0104 - acc: 0.9956 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 136/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0119 - acc: 0.9974 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 137/500\n",
            "1149/1149 [==============================] - 1s 930us/sample - loss: 0.0383 - acc: 0.9896 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 138/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 139/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 140/500\n",
            "1149/1149 [==============================] - 1s 843us/sample - loss: 0.0050 - acc: 0.9991 - val_loss: 8.8055e-04 - val_acc: 1.0000\n",
            "Epoch 141/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 6.9072e-04 - val_acc: 1.0000\n",
            "Epoch 142/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 3.5790e-04 - val_acc: 1.0000\n",
            "Epoch 143/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 8.2297e-04 - val_acc: 1.0000\n",
            "Epoch 144/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 1.8192e-04 - val_acc: 1.0000\n",
            "Epoch 145/500\n",
            "1149/1149 [==============================] - 1s 860us/sample - loss: 0.0016 - acc: 0.9991 - val_loss: 1.7942e-04 - val_acc: 1.0000\n",
            "Epoch 146/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 1.7045e-04 - val_acc: 1.0000\n",
            "Epoch 147/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7367e-04 - val_acc: 1.0000\n",
            "Epoch 148/500\n",
            "1149/1149 [==============================] - 1s 890us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 4.6163e-04 - val_acc: 1.0000\n",
            "Epoch 149/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 1.6913e-04 - val_acc: 1.0000\n",
            "Epoch 150/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 6.0901e-04 - acc: 1.0000 - val_loss: 1.0837e-04 - val_acc: 1.0000\n",
            "Epoch 151/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 4.7051e-04 - acc: 1.0000 - val_loss: 1.0294e-04 - val_acc: 1.0000\n",
            "Epoch 152/500\n",
            "1149/1149 [==============================] - 1s 883us/sample - loss: 7.4612e-04 - acc: 1.0000 - val_loss: 8.6557e-05 - val_acc: 1.0000\n",
            "Epoch 153/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 8.4021e-05 - val_acc: 1.0000\n",
            "Epoch 154/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 4.4050e-04 - acc: 1.0000 - val_loss: 6.8618e-05 - val_acc: 1.0000\n",
            "Epoch 155/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 7.6267e-05 - val_acc: 1.0000\n",
            "Epoch 156/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 7.3192e-05 - val_acc: 1.0000\n",
            "Epoch 157/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0044 - acc: 0.9991 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 158/500\n",
            "1149/1149 [==============================] - 1s 832us/sample - loss: 0.0303 - acc: 0.9887 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 159/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.2189 - acc: 0.9086 - val_loss: 0.0583 - val_acc: 0.9739\n",
            "Epoch 160/500\n",
            "1149/1149 [==============================] - 1s 844us/sample - loss: 0.1193 - acc: 0.9487 - val_loss: 0.0539 - val_acc: 0.9869\n",
            "Epoch 161/500\n",
            "1149/1149 [==============================] - 1s 834us/sample - loss: 0.0667 - acc: 0.9756 - val_loss: 0.0356 - val_acc: 0.9817\n",
            "Epoch 162/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.1076 - acc: 0.9565 - val_loss: 0.0378 - val_acc: 0.9948\n",
            "Epoch 163/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0545 - acc: 0.9826 - val_loss: 0.0158 - val_acc: 0.9948\n",
            "Epoch 164/500\n",
            "1149/1149 [==============================] - 1s 838us/sample - loss: 0.0199 - acc: 0.9922 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 165/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.0099 - acc: 0.9974 - val_loss: 0.0060 - val_acc: 0.9974\n",
            "Epoch 166/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.0117 - acc: 0.9939 - val_loss: 0.0047 - val_acc: 1.0000\n",
            "Epoch 167/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0330 - acc: 0.9887 - val_loss: 0.0439 - val_acc: 0.9817\n",
            "Epoch 168/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0134 - acc: 0.9948 - val_loss: 0.0060 - val_acc: 0.9974\n",
            "Epoch 169/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0324 - acc: 0.9896 - val_loss: 0.0049 - val_acc: 1.0000\n",
            "Epoch 170/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0210 - acc: 0.9913 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 171/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0599 - acc: 0.9782 - val_loss: 0.0365 - val_acc: 0.9869\n",
            "Epoch 172/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.2373 - acc: 0.9339 - val_loss: 0.0643 - val_acc: 1.0000\n",
            "Epoch 173/500\n",
            "1149/1149 [==============================] - 1s 859us/sample - loss: 0.0571 - acc: 0.9852 - val_loss: 0.0084 - val_acc: 1.0000\n",
            "Epoch 174/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0697 - acc: 0.9721 - val_loss: 0.0171 - val_acc: 0.9974\n",
            "Epoch 175/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0452 - acc: 0.9826 - val_loss: 0.0091 - val_acc: 1.0000\n",
            "Epoch 176/500\n",
            "1149/1149 [==============================] - 1s 877us/sample - loss: 0.0453 - acc: 0.9800 - val_loss: 0.0247 - val_acc: 0.9948\n",
            "Epoch 177/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0327 - acc: 0.9896 - val_loss: 0.0277 - val_acc: 0.9896\n",
            "Epoch 178/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0336 - acc: 0.9869 - val_loss: 0.0059 - val_acc: 1.0000\n",
            "Epoch 179/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0528 - acc: 0.9826 - val_loss: 0.0780 - val_acc: 0.9634\n",
            "Epoch 180/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.1149 - acc: 0.9591 - val_loss: 0.0178 - val_acc: 0.9974\n",
            "Epoch 181/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 0.0306 - acc: 0.9896 - val_loss: 0.0060 - val_acc: 1.0000\n",
            "Epoch 182/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0310 - acc: 0.9861 - val_loss: 0.0029 - val_acc: 1.0000\n",
            "Epoch 183/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0859 - acc: 0.9695 - val_loss: 0.0928 - val_acc: 0.9582\n",
            "Epoch 184/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 0.0592 - acc: 0.9739 - val_loss: 0.0040 - val_acc: 1.0000\n",
            "Epoch 185/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0136 - acc: 0.9974 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 186/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0161 - acc: 0.9965 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 187/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0195 - acc: 0.9948 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 188/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0068 - acc: 0.9983 - val_loss: 0.0056 - val_acc: 0.9974\n",
            "Epoch 189/500\n",
            "1149/1149 [==============================] - 1s 894us/sample - loss: 0.0096 - acc: 0.9956 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 190/500\n",
            "1149/1149 [==============================] - 1s 855us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0054 - val_acc: 0.9974\n",
            "Epoch 191/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 192/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0114 - acc: 0.9948 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 193/500\n",
            "1149/1149 [==============================] - 1s 899us/sample - loss: 0.0076 - acc: 0.9965 - val_loss: 8.7846e-04 - val_acc: 1.0000\n",
            "Epoch 194/500\n",
            "1149/1149 [==============================] - 1s 886us/sample - loss: 0.0079 - acc: 0.9983 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 195/500\n",
            "1149/1149 [==============================] - 1s 892us/sample - loss: 0.0585 - acc: 0.9843 - val_loss: 0.0202 - val_acc: 0.9948\n",
            "Epoch 196/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0799 - acc: 0.9695 - val_loss: 0.0199 - val_acc: 0.9948\n",
            "Epoch 197/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 0.0529 - acc: 0.9817 - val_loss: 0.0181 - val_acc: 0.9974\n",
            "Epoch 198/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0303 - acc: 0.9913 - val_loss: 0.0081 - val_acc: 0.9948\n",
            "Epoch 199/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0104 - acc: 0.9983 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 200/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0066 - acc: 0.9991 - val_loss: 8.6367e-04 - val_acc: 1.0000\n",
            "Epoch 201/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0065 - acc: 0.9983 - val_loss: 0.0028 - val_acc: 0.9974\n",
            "Epoch 202/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0101 - acc: 0.9956 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 203/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0119 - acc: 0.9965 - val_loss: 9.9985e-04 - val_acc: 1.0000\n",
            "Epoch 204/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0270 - acc: 0.9887 - val_loss: 0.0455 - val_acc: 0.9765\n",
            "Epoch 205/500\n",
            "1149/1149 [==============================] - 1s 890us/sample - loss: 0.0365 - acc: 0.9878 - val_loss: 0.0147 - val_acc: 0.9948\n",
            "Epoch 206/500\n",
            "1149/1149 [==============================] - 1s 877us/sample - loss: 0.0623 - acc: 0.9782 - val_loss: 0.0509 - val_acc: 0.9791\n",
            "Epoch 207/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0494 - acc: 0.9817 - val_loss: 0.0852 - val_acc: 0.9634\n",
            "Epoch 208/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0389 - acc: 0.9861 - val_loss: 0.0049 - val_acc: 0.9974\n",
            "Epoch 209/500\n",
            "1149/1149 [==============================] - 1s 901us/sample - loss: 0.0287 - acc: 0.9904 - val_loss: 0.0027 - val_acc: 1.0000\n",
            "Epoch 210/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0343 - acc: 0.9887 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 211/500\n",
            "1149/1149 [==============================] - 1s 893us/sample - loss: 0.0795 - acc: 0.9713 - val_loss: 0.0204 - val_acc: 1.0000\n",
            "Epoch 212/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0180 - acc: 0.9939 - val_loss: 0.0034 - val_acc: 1.0000\n",
            "Epoch 213/500\n",
            "1149/1149 [==============================] - 1s 859us/sample - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 214/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0054 - acc: 0.9991 - val_loss: 3.6252e-04 - val_acc: 1.0000\n",
            "Epoch 215/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 2.1614e-04 - val_acc: 1.0000\n",
            "Epoch 216/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 7.5693e-04 - val_acc: 1.0000\n",
            "Epoch 217/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0055 - val_acc: 0.9974\n",
            "Epoch 218/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 3.9457e-04 - val_acc: 1.0000\n",
            "Epoch 219/500\n",
            "1149/1149 [==============================] - 1s 884us/sample - loss: 0.0061 - acc: 0.9991 - val_loss: 4.3129e-04 - val_acc: 1.0000\n",
            "Epoch 220/500\n",
            "1149/1149 [==============================] - 1s 888us/sample - loss: 0.0045 - acc: 0.9983 - val_loss: 1.2953e-04 - val_acc: 1.0000\n",
            "Epoch 221/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 2.4867e-04 - val_acc: 1.0000\n",
            "Epoch 222/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 4.1413e-04 - val_acc: 1.0000\n",
            "Epoch 223/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 4.6459e-04 - val_acc: 1.0000\n",
            "Epoch 224/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0071 - acc: 0.9965 - val_loss: 0.0064 - val_acc: 0.9974\n",
            "Epoch 225/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0264 - acc: 0.9896 - val_loss: 0.0714 - val_acc: 0.9843\n",
            "Epoch 226/500\n",
            "1149/1149 [==============================] - 1s 856us/sample - loss: 0.0781 - acc: 0.9730 - val_loss: 0.0272 - val_acc: 0.9922\n",
            "Epoch 227/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0536 - acc: 0.9826 - val_loss: 0.0312 - val_acc: 0.9948\n",
            "Epoch 228/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0245 - acc: 0.9904 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 229/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 9.0325e-04 - val_acc: 1.0000\n",
            "Epoch 230/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0050 - acc: 0.9991 - val_loss: 6.3290e-04 - val_acc: 1.0000\n",
            "Epoch 231/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 3.2604e-04 - val_acc: 1.0000\n",
            "Epoch 232/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0027 - acc: 0.9991 - val_loss: 4.8857e-04 - val_acc: 1.0000\n",
            "Epoch 233/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.0129 - acc: 0.9948 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 234/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.2176 - acc: 0.9234 - val_loss: 0.4280 - val_acc: 0.7755\n",
            "Epoch 235/500\n",
            "1149/1149 [==============================] - 1s 892us/sample - loss: 0.1979 - acc: 0.9112 - val_loss: 0.0495 - val_acc: 0.9869\n",
            "Epoch 236/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.1161 - acc: 0.9530 - val_loss: 0.0933 - val_acc: 0.9504\n",
            "Epoch 237/500\n",
            "1149/1149 [==============================] - 1s 842us/sample - loss: 0.1007 - acc: 0.9547 - val_loss: 0.0331 - val_acc: 0.9922\n",
            "Epoch 238/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 0.0568 - acc: 0.9843 - val_loss: 0.0481 - val_acc: 0.9791\n",
            "Epoch 239/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0478 - acc: 0.9887 - val_loss: 0.0204 - val_acc: 0.9948\n",
            "Epoch 240/500\n",
            "1149/1149 [==============================] - 1s 857us/sample - loss: 0.0395 - acc: 0.9852 - val_loss: 0.0981 - val_acc: 0.9634\n",
            "Epoch 241/500\n",
            "1149/1149 [==============================] - 1s 837us/sample - loss: 0.1248 - acc: 0.9452 - val_loss: 0.0437 - val_acc: 0.9974\n",
            "Epoch 242/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0830 - acc: 0.9695 - val_loss: 0.1003 - val_acc: 0.9661\n",
            "Epoch 243/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0858 - acc: 0.9695 - val_loss: 0.0222 - val_acc: 1.0000\n",
            "Epoch 244/500\n",
            "1149/1149 [==============================] - 1s 850us/sample - loss: 0.0414 - acc: 0.9896 - val_loss: 0.0112 - val_acc: 1.0000\n",
            "Epoch 245/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0076 - val_acc: 1.0000\n",
            "Epoch 246/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0233 - acc: 0.9922 - val_loss: 0.0076 - val_acc: 1.0000\n",
            "Epoch 247/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0056 - val_acc: 1.0000\n",
            "Epoch 248/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0130 - acc: 0.9983 - val_loss: 0.0050 - val_acc: 1.0000\n",
            "Epoch 249/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0106 - acc: 0.9983 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 250/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0195 - acc: 0.9922 - val_loss: 0.0178 - val_acc: 0.9896\n",
            "Epoch 251/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0189 - acc: 0.9948 - val_loss: 0.0128 - val_acc: 1.0000\n",
            "Epoch 252/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0038 - val_acc: 1.0000\n",
            "Epoch 253/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0036 - val_acc: 1.0000\n",
            "Epoch 254/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 255/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0087 - acc: 0.9983 - val_loss: 0.0043 - val_acc: 1.0000\n",
            "Epoch 256/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0071 - acc: 0.9991 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 257/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0075 - val_acc: 0.9974\n",
            "Epoch 258/500\n",
            "1149/1149 [==============================] - 1s 884us/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 259/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0047 - acc: 0.9991 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 260/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 261/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0181 - val_acc: 0.9896\n",
            "Epoch 262/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0149 - acc: 0.9939 - val_loss: 0.0067 - val_acc: 1.0000\n",
            "Epoch 263/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0027 - val_acc: 1.0000\n",
            "Epoch 264/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 265/500\n",
            "1149/1149 [==============================] - 1s 851us/sample - loss: 0.0095 - acc: 0.9948 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 266/500\n",
            "1149/1149 [==============================] - 1s 855us/sample - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 267/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 268/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 269/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0039 - acc: 0.9983 - val_loss: 0.0012 - val_acc: 1.0000\n",
            "Epoch 270/500\n",
            "1149/1149 [==============================] - 1s 859us/sample - loss: 0.0065 - acc: 0.9974 - val_loss: 0.0097 - val_acc: 1.0000\n",
            "Epoch 271/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.1163 - acc: 0.9704 - val_loss: 0.0467 - val_acc: 0.9869\n",
            "Epoch 272/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.1181 - acc: 0.9530 - val_loss: 0.0585 - val_acc: 0.9843\n",
            "Epoch 273/500\n",
            "1149/1149 [==============================] - 1s 886us/sample - loss: 0.0372 - acc: 0.9887 - val_loss: 0.0083 - val_acc: 1.0000\n",
            "Epoch 274/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "Epoch 275/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0108 - acc: 0.9956 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 276/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0066 - acc: 0.9991 - val_loss: 0.0021 - val_acc: 1.0000\n",
            "Epoch 277/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 278/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0053 - acc: 0.9991 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 279/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0011 - val_acc: 1.0000\n",
            "Epoch 280/500\n",
            "1149/1149 [==============================] - 1s 888us/sample - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0083 - val_acc: 1.0000\n",
            "Epoch 281/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 0.0109 - acc: 0.9983 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 282/500\n",
            "1149/1149 [==============================] - 1s 887us/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 7.2552e-04 - val_acc: 1.0000\n",
            "Epoch 283/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 284/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 285/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 286/500\n",
            "1149/1149 [==============================] - 1s 877us/sample - loss: 0.0040 - acc: 0.9983 - val_loss: 6.3252e-04 - val_acc: 1.0000\n",
            "Epoch 287/500\n",
            "1149/1149 [==============================] - 1s 911us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 288/500\n",
            "1149/1149 [==============================] - 1s 892us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 3.6851e-04 - val_acc: 1.0000\n",
            "Epoch 289/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.1699 - acc: 0.9721 - val_loss: 0.0127 - val_acc: 0.9948\n",
            "Epoch 290/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 0.2192 - acc: 0.9339 - val_loss: 0.0974 - val_acc: 0.9687\n",
            "Epoch 291/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.0616 - acc: 0.9852 - val_loss: 0.0152 - val_acc: 1.0000\n",
            "Epoch 292/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0346 - acc: 0.9904 - val_loss: 0.0185 - val_acc: 1.0000\n",
            "Epoch 293/500\n",
            "1149/1149 [==============================] - 1s 880us/sample - loss: 0.0217 - acc: 0.9922 - val_loss: 0.0072 - val_acc: 0.9974\n",
            "Epoch 294/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0185 - val_acc: 0.9896\n",
            "Epoch 295/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0231 - acc: 0.9922 - val_loss: 0.0085 - val_acc: 0.9974\n",
            "Epoch 296/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 0.0244 - acc: 0.9913 - val_loss: 0.0049 - val_acc: 1.0000\n",
            "Epoch 297/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 0.0272 - acc: 0.9956 - val_loss: 0.0069 - val_acc: 1.0000\n",
            "Epoch 298/500\n",
            "1149/1149 [==============================] - 1s 888us/sample - loss: 0.0227 - acc: 0.9930 - val_loss: 0.0050 - val_acc: 1.0000\n",
            "Epoch 299/500\n",
            "1149/1149 [==============================] - 1s 892us/sample - loss: 0.0346 - acc: 0.9878 - val_loss: 0.0218 - val_acc: 0.9896\n",
            "Epoch 300/500\n",
            "1149/1149 [==============================] - 1s 849us/sample - loss: 0.0217 - acc: 0.9939 - val_loss: 0.0030 - val_acc: 1.0000\n",
            "Epoch 301/500\n",
            "1149/1149 [==============================] - 1s 860us/sample - loss: 0.0098 - acc: 0.9991 - val_loss: 0.0066 - val_acc: 0.9974\n",
            "Epoch 302/500\n",
            "1149/1149 [==============================] - 1s 890us/sample - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 303/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 304/500\n",
            "1149/1149 [==============================] - 1s 886us/sample - loss: 0.0097 - acc: 0.9974 - val_loss: 0.0073 - val_acc: 1.0000\n",
            "Epoch 305/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 306/500\n",
            "1149/1149 [==============================] - 1s 876us/sample - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 307/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 308/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 309/500\n",
            "1149/1149 [==============================] - 1s 891us/sample - loss: 0.0054 - acc: 0.9983 - val_loss: 9.9189e-04 - val_acc: 1.0000\n",
            "Epoch 310/500\n",
            "1149/1149 [==============================] - 1s 870us/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 311/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 0.0066 - acc: 0.9983 - val_loss: 0.0057 - val_acc: 0.9974\n",
            "Epoch 312/500\n",
            "1149/1149 [==============================] - 1s 857us/sample - loss: 0.0085 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 313/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0080 - acc: 0.9965 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 314/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0065 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 315/500\n",
            "1149/1149 [==============================] - 1s 884us/sample - loss: 0.0087 - acc: 0.9983 - val_loss: 4.8320e-04 - val_acc: 1.0000\n",
            "Epoch 316/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0259 - acc: 0.9904 - val_loss: 0.0034 - val_acc: 1.0000\n",
            "Epoch 317/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 0.0177 - acc: 0.9939 - val_loss: 0.0142 - val_acc: 0.9896\n",
            "Epoch 318/500\n",
            "1149/1149 [==============================] - 1s 836us/sample - loss: 0.0683 - acc: 0.9809 - val_loss: 0.0398 - val_acc: 0.9869\n",
            "Epoch 319/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0327 - acc: 0.9896 - val_loss: 0.0054 - val_acc: 1.0000\n",
            "Epoch 320/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 0.0105 - acc: 0.9983 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 321/500\n",
            "1149/1149 [==============================] - 1s 885us/sample - loss: 0.0064 - acc: 0.9983 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 322/500\n",
            "1149/1149 [==============================] - 1s 881us/sample - loss: 0.0234 - acc: 0.9913 - val_loss: 0.0271 - val_acc: 0.9896\n",
            "Epoch 323/500\n",
            "1149/1149 [==============================] - 1s 877us/sample - loss: 0.0445 - acc: 0.9835 - val_loss: 0.0039 - val_acc: 1.0000\n",
            "Epoch 324/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 0.0325 - acc: 0.9843 - val_loss: 0.1336 - val_acc: 0.9530\n",
            "Epoch 325/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0497 - acc: 0.9826 - val_loss: 0.0259 - val_acc: 0.9922\n",
            "Epoch 326/500\n",
            "1149/1149 [==============================] - 1s 863us/sample - loss: 0.0338 - acc: 0.9887 - val_loss: 0.0190 - val_acc: 0.9922\n",
            "Epoch 327/500\n",
            "1149/1149 [==============================] - 1s 852us/sample - loss: 0.0606 - acc: 0.9809 - val_loss: 0.0051 - val_acc: 1.0000\n",
            "Epoch 328/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0306 - acc: 0.9922 - val_loss: 0.0170 - val_acc: 0.9948\n",
            "Epoch 329/500\n",
            "1149/1149 [==============================] - 1s 847us/sample - loss: 0.0148 - acc: 0.9974 - val_loss: 0.0176 - val_acc: 0.9948\n",
            "Epoch 330/500\n",
            "1149/1149 [==============================] - 1s 860us/sample - loss: 0.0090 - acc: 0.9983 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 331/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 332/500\n",
            "1149/1149 [==============================] - 1s 866us/sample - loss: 0.0072 - acc: 0.9991 - val_loss: 0.0013 - val_acc: 1.0000\n",
            "Epoch 333/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 0.0096 - acc: 0.9956 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 334/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 0.0133 - acc: 0.9930 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 335/500\n",
            "1149/1149 [==============================] - 1s 867us/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 9.3842e-04 - val_acc: 1.0000\n",
            "Epoch 336/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 6.7919e-04 - val_acc: 1.0000\n",
            "Epoch 337/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 3.5360e-04 - val_acc: 1.0000\n",
            "Epoch 338/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0038 - acc: 0.9991 - val_loss: 3.7874e-04 - val_acc: 1.0000\n",
            "Epoch 339/500\n",
            "1149/1149 [==============================] - 1s 872us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 3.7612e-04 - val_acc: 1.0000\n",
            "Epoch 340/500\n",
            "1149/1149 [==============================] - 1s 854us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 3.1790e-04 - val_acc: 1.0000\n",
            "Epoch 341/500\n",
            "1149/1149 [==============================] - 1s 879us/sample - loss: 6.0186e-04 - acc: 1.0000 - val_loss: 1.5080e-04 - val_acc: 1.0000\n",
            "Epoch 342/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 7.0734e-04 - acc: 1.0000 - val_loss: 1.7521e-04 - val_acc: 1.0000\n",
            "Epoch 343/500\n",
            "1149/1149 [==============================] - 1s 878us/sample - loss: 6.5545e-04 - acc: 1.0000 - val_loss: 1.0908e-04 - val_acc: 1.0000\n",
            "Epoch 344/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 4.2663e-04 - acc: 1.0000 - val_loss: 1.1898e-04 - val_acc: 1.0000\n",
            "Epoch 345/500\n",
            "1149/1149 [==============================] - 1s 871us/sample - loss: 4.7200e-04 - acc: 1.0000 - val_loss: 7.9307e-05 - val_acc: 1.0000\n",
            "Epoch 346/500\n",
            "1149/1149 [==============================] - 1s 858us/sample - loss: 4.6245e-04 - acc: 1.0000 - val_loss: 8.3860e-05 - val_acc: 1.0000\n",
            "Epoch 347/500\n",
            "1149/1149 [==============================] - 1s 862us/sample - loss: 7.3770e-04 - acc: 1.0000 - val_loss: 1.6015e-04 - val_acc: 1.0000\n",
            "Epoch 348/500\n",
            "1149/1149 [==============================] - 1s 865us/sample - loss: 9.2160e-04 - acc: 1.0000 - val_loss: 7.3378e-05 - val_acc: 1.0000\n",
            "Epoch 349/500\n",
            "1149/1149 [==============================] - 1s 869us/sample - loss: 5.8010e-04 - acc: 1.0000 - val_loss: 9.0776e-05 - val_acc: 1.0000\n",
            "Epoch 350/500\n",
            "1149/1149 [==============================] - 1s 859us/sample - loss: 4.1512e-04 - acc: 1.0000 - val_loss: 7.8831e-05 - val_acc: 1.0000\n",
            "Epoch 351/500\n",
            "1149/1149 [==============================] - 1s 864us/sample - loss: 3.3554e-04 - acc: 1.0000 - val_loss: 6.0520e-05 - val_acc: 1.0000\n",
            "Epoch 352/500\n",
            "1149/1149 [==============================] - 1s 857us/sample - loss: 3.5079e-04 - acc: 1.0000 - val_loss: 6.2693e-05 - val_acc: 1.0000\n",
            "Epoch 353/500\n",
            "1149/1149 [==============================] - 1s 883us/sample - loss: 2.5309e-04 - acc: 1.0000 - val_loss: 4.1371e-05 - val_acc: 1.0000\n",
            "Epoch 354/500\n",
            "1149/1149 [==============================] - 1s 874us/sample - loss: 5.2815e-04 - acc: 1.0000 - val_loss: 3.9032e-05 - val_acc: 1.0000\n",
            "Epoch 355/500\n",
            "1149/1149 [==============================] - 1s 900us/sample - loss: 4.3056e-04 - acc: 1.0000 - val_loss: 6.4050e-05 - val_acc: 1.0000\n",
            "Epoch 356/500\n",
            "1149/1149 [==============================] - 1s 868us/sample - loss: 0.0042 - acc: 0.9974 - val_loss: 0.0081 - val_acc: 0.9974\n",
            "Epoch 357/500\n",
            "1149/1149 [==============================] - 1s 873us/sample - loss: 0.0239 - acc: 0.9913 - val_loss: 0.0593 - val_acc: 0.9843\n",
            "Epoch 358/500\n",
            "1149/1149 [==============================] - 1s 882us/sample - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0023 - val_acc: 1.0000\n",
            "Epoch 359/500\n",
            "1149/1149 [==============================] - 1s 861us/sample - loss: 0.0075 - acc: 0.9965 - val_loss: 2.5536e-04 - val_acc: 1.0000\n",
            "Epoch 360/500\n",
            "1149/1149 [==============================] - 1s 845us/sample - loss: 0.0049 - acc: 0.9991 - val_loss: 0.0025 - val_acc: 1.0000\n",
            "Epoch 361/500\n",
            "1149/1149 [==============================] - 1s 846us/sample - loss: 0.0519 - acc: 0.9826 - val_loss: 0.0090 - val_acc: 1.0000\n",
            "Epoch 362/500\n",
            "1149/1149 [==============================] - 1s 853us/sample - loss: 0.0179 - acc: 0.9948 - val_loss: 0.0092 - val_acc: 0.9974\n",
            "Epoch 363/500\n",
            "1149/1149 [==============================] - 1s 875us/sample - loss: 0.0159 - acc: 0.9939 - val_loss: 0.0138 - val_acc: 0.9948\n",
            "Epoch 364/500\n",
            " 320/1149 [=======>......................] - ETA: 0s - loss: 0.0088 - acc: 0.9969"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxJE9hTShdlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT17sarbeBsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def cnn_model_function(input_shape, filter_num, kernel_size):\n",
        "#   model = Sequential()\n",
        "#   #Layer 1\n",
        "#   #Conv Layer 1\n",
        "#   model.add(Conv2D(filters = filter_num[0], \n",
        "#                    kernel_size = kernel_size[0], \n",
        "#                    strides = 1, \n",
        "#                    activation = 'relu', \n",
        "#                    input_shape = (input_shape,input_shape,3)))\n",
        "#   #Pooling layer 1\n",
        "#   model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "#   #Layer 2\n",
        "#   #Conv Layer 2\n",
        "#   model.add(Conv2D(filters = filter_num[1], \n",
        "#                    kernel_size = kernel_size[0],\n",
        "#                    strides = 1,\n",
        "#                    activation = 'relu',\n",
        "#                    input_shape = (input_shape/2,input_shape/2,6)))\n",
        "#   #Pooling Layer 2\n",
        "#   model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
        "#   #Flatten\n",
        "#   model.add(Flatten())\n",
        "#   return model\n",
        "\n",
        "# def cnn_model_function(input_shape, filter_num, kernel_size):\n",
        "#   input_layer = Input(shape=(input_shape, input_shape,3,))\n",
        "#   #Layer 1\n",
        "#   #Conv Layer 1\n",
        "#   conv_layer1 = Conv2D(filters = filter_num[0], \n",
        "#                    kernel_size = kernel_size[0], \n",
        "#                    strides = 1, \n",
        "#                    activation = 'relu')(input_layer)\n",
        "#   #Pooling layer 1\n",
        "#   max_layer1 = MaxPooling2D(pool_size = 2, strides = 2)(conv_layer1)\n",
        "#   #Layer 2\n",
        "#   #Conv Layer 2\n",
        "#   conv_layer2 = Conv2D(filters = filter_num[1], \n",
        "#                    kernel_size = kernel_size[0], \n",
        "#                    strides = 1, \n",
        "#                    activation = 'relu')(max_layer1)\n",
        "#   #Pooling Layer 2\n",
        "#   max_layer2 = MaxPooling2D(pool_size = 2, strides = 2)(conv_layer2)\n",
        "#   #Flatten\n",
        "#   flat = Flatten()(max_layer2)  \n",
        "#   return flat, input_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75TiXG3xdSwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Double CNN\n",
        "\n",
        "# train_data1, train_label1, test_data1, test_label1 = load_data(128)\n",
        "# train_data2, train_label2, test_data2, test_label2 = load_data(64)\n",
        "\n",
        "# random.seed(31)\n",
        "# cnn1, input1 = cnn_model_function(128, [64, 256], [36, 2])\n",
        "# cnn2, input2 = cnn_model_function(64, [32, 128], [12, 3])\n",
        "\n",
        "# #concat_layer = Concatenate([cnn1, cnn2])\n",
        "# concat_layer = keras.layers.concatenate([cnn1, cnn2])\n",
        "\n",
        "\n",
        "# fully_connected1 = Dense(units = 1000, activation = 'relu')(concat_layer)\n",
        "# fully_connected2 = Dense(units = 512, activation = 'relu')(fully_connected1)\n",
        "\n",
        "# #Layer 5\n",
        "# #Output Layer\n",
        "# fully_connected3 = Dense(units = 3, activation = 'softmax')(fully_connected2)\n",
        "# model = Model(inputs=[input1, input2], outputs=fully_connected3)\n",
        "# model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# model.fit(([train_data1, train_data2]) ,train_label1, batch_size = 64, epochs = 100, validation_data=(([test_data1, test_data2]),test_label1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AzbBtDuzP7P",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}